{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from tqdm.auto import tqdm\n",
        "from glob import glob\n",
        "import time, gc\n",
        "import cv2\n",
        "import tensorflow\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.layers import Embedding\n",
        "import matplotlib.image as mpimg\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.models import clone_model\n",
        "from tensorflow.keras.layers import Dense,Conv2D,Flatten,MaxPool2D,Dropout,BatchNormalization, Input, Activation\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import PIL.Image as Image, PIL.ImageDraw as ImageDraw, PIL.ImageFont as ImageFont\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import albumentations as A"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will soon switch to TensorFlow 2.x.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now \n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ],
      "execution_count": 1,
      "metadata": {
        "id": "MKmXagSQie7i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 80
        },
        "outputId": "8f0eab84-747f-4032-ffde-90e5e3cdcbf6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ResidualUnit(tensorflow.keras.layers.Layer):\n",
        "  def __init__(self, filters, model, strides=1, activation=\"relu\", **kwargs):\n",
        "    super().__init__(**kwargs)\n",
        "    self.activation = tensorflow.keras.activations.get(activation)\n",
        "    self.main_layers = [\n",
        "        tensorflow.keras.layers.Conv2D(filters, 3, strides=strides,\n",
        "        padding=\"same\", use_bias=False),\n",
        "        tensorflow.keras.layers.BatchNormalization(),\n",
        "        self.activation,\n",
        "        tensorflow.keras.layers.Conv2D(filters, 3, strides=1,\n",
        "        padding=\"same\", use_bias=False),\n",
        "        tensorflow.keras.layers.BatchNormalization()]\n",
        "\n",
        "    self.skip_layers = []\n",
        "    if strides > 1:\n",
        "      self.skip_layers = [\n",
        "        tensorflow.keras.layers.Conv2D(filters, 1, strides=strides,\n",
        "        padding=\"same\", use_bias=False),\n",
        "        tensorflow.keras.layers.BatchNormalization()]\n",
        "\n",
        "    self.model = self.call(model)\n",
        "\n",
        "  def call(self, inputs):\n",
        "    Z = inputs\n",
        "    for layer in self.main_layers:\n",
        "      Z = layer(Z)\n",
        "    skip_Z = inputs\n",
        "    for layer in self.skip_layers:\n",
        "      skip_Z = layer(skip_Z)\n",
        "    return self.activation(Z + skip_Z)\n",
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "-0hS0JGLkXg1",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "QOUkbNGqkjKW",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_df_ = pd.read_csv('drive/My Drive/Colab Notebooks/playground/train.csv')\n",
        "sample_sub_df = pd.read_csv('drive/My Drive/Colab Notebooks/playground/sample_submission.csv')\n",
        "\n",
        "# not really useful, just for human understanding\n",
        "class_map_df = pd.read_csv('drive/My Drive/Colab Notebooks/playground/class_map.csv')"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "-xb2ZYi1lr-v",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "code",
      "source": [
        "HEIGHT = 137\n",
        "WIDTH = 236\n"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "BfMKrHSYmVHA",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def resize(df, size=64, need_progress_bar=True):\n",
        "    resized = {}\n",
        "    resize_size=64\n",
        "    angle=0\n",
        "    if need_progress_bar:\n",
        "        for i in tqdm(range(df.shape[0])):\n",
        "            #image = cv2.resize(df.loc[df.index[i]].values.reshape(137,236),(size,size),None,fx=0.5,fy=0.5,interpolation=cv2.INTER_AREA)\n",
        "            image=df.loc[df.index[i]].values.reshape(137,236)\n",
        "            #Centering\n",
        "            image_center = tuple(np.array(image.shape[1::-1]) / 2)\n",
        "            matrix = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n",
        "            image = cv2.warpAffine(image, matrix, image.shape[1::-1], flags=cv2.INTER_LINEAR,\n",
        "                            borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0))\n",
        "            #Scaling\n",
        "            matrix = cv2.getRotationMatrix2D(image_center, 0, 1.0)\n",
        "            image = cv2.warpAffine(image, matrix, image.shape[1::-1], flags=cv2.INTER_LINEAR,\n",
        "                            borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0))\n",
        "            #Removing Blur\n",
        "            #aug = A.GaussianBlur(p=1.0)\n",
        "            #image = aug(image=image)['image']\n",
        "            #Noise Removing\n",
        "            #augNoise=A.MultiplicativeNoise(p=1.0)\n",
        "            #image = augNoise(image=image)['image']\n",
        "            #Removing Distortion\n",
        "            #augDist=A.ElasticTransform(sigma=50, alpha=1, alpha_affine=10, p=1.0)\n",
        "            #image = augDist(image=image)['image']\n",
        "            #Brightness\n",
        "            augBright=A.RandomBrightnessContrast(p=1.0)\n",
        "            image = augBright(image=image)['image']\n",
        "            _, thresh = cv2.threshold(image, 30, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "            contours, _ = cv2.findContours(thresh,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)[-2:]\n",
        "\n",
        "            idx = 0 \n",
        "            ls_xmin = []\n",
        "            ls_ymin = []\n",
        "            ls_xmax = []\n",
        "            ls_ymax = []\n",
        "            for cnt in contours:\n",
        "                idx += 1\n",
        "                x,y,w,h = cv2.boundingRect(cnt)\n",
        "                ls_xmin.append(x)\n",
        "                ls_ymin.append(y)\n",
        "                ls_xmax.append(x + w)\n",
        "                ls_ymax.append(y + h)\n",
        "            xmin = min(ls_xmin)\n",
        "            ymin = min(ls_ymin)\n",
        "            xmax = max(ls_xmax)\n",
        "            ymax = max(ls_ymax)\n",
        "\n",
        "            roi = image[ymin:ymax,xmin:xmax]\n",
        "            resized_roi = cv2.resize(roi, (resize_size, resize_size),interpolation=cv2.INTER_AREA)\n",
        "            #image=affine_image(image)\n",
        "            #image= crop_resize(image)\n",
        "            #image = cv2.resize(image,(size,size),interpolation=cv2.INTER_AREA)\n",
        "            #image=resize_image(image,(64,64))\n",
        "            #image = cv2.resize(image,(size,size),interpolation=cv2.INTER_AREA)\n",
        "            #gaussian_3 = cv2.GaussianBlur(image, (5,5), cv2.BORDER_DEFAULT) #unblur\n",
        "            #image = cv2.addWeighted(image, 1.5, gaussian_3, -0.5, 0, image)\n",
        "            #kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]]) #filter\n",
        "            #image = cv2.filter2D(image, -1, kernel)\n",
        "            #ret,image = cv2.threshold(image, 128, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
        "            resized[df.index[i]] = resized_roi.reshape(-1)\n",
        "    else:\n",
        "        for i in range(df.shape[0]):\n",
        "            #image = cv2.resize(df.loc[df.index[i]].values.reshape(137,236),(size,size),None,fx=0.5,fy=0.5,interpolation=cv2.INTER_AREA)\n",
        "            image=df.loc[df.index[i]].values.reshape(137,236)\n",
        "            image_center = tuple(np.array(image.shape[1::-1]) / 2)\n",
        "            matrix = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n",
        "            image = cv2.warpAffine(image, matrix, image.shape[1::-1], flags=cv2.INTER_LINEAR,\n",
        "                            borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0))\n",
        "            matrix = cv2.getRotationMatrix2D(image_center, 0, 1.0)\n",
        "            image = cv2.warpAffine(image, matrix, image.shape[1::-1], flags=cv2.INTER_LINEAR,\n",
        "                            borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0))\n",
        "            #Removing Blur\n",
        "            #aug = A.GaussianBlur(p=1.0)\n",
        "            #image = aug(image=image)['image']\n",
        "            #Noise Removing\n",
        "            #augNoise=A.MultiplicativeNoise(p=1.0)\n",
        "            #image = augNoise(image=image)['image']\n",
        "            #Removing Distortion\n",
        "            #augDist=A.ElasticTransform(sigma=50, alpha=1, alpha_affine=10, p=1.0)\n",
        "            #image = augDist(image=image)['image']\n",
        "            #Brightness\n",
        "            augBright=A.RandomBrightnessContrast(p=1.0)\n",
        "            image = augBright(image=image)['image']\n",
        "            _, thresh = cv2.threshold(image, 30, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "            contours, _ = cv2.findContours(thresh,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)[-2:]\n",
        "\n",
        "            idx = 0 \n",
        "            ls_xmin = []\n",
        "            ls_ymin = []\n",
        "            ls_xmax = []\n",
        "            ls_ymax = []\n",
        "            for cnt in contours:\n",
        "                idx += 1\n",
        "                x,y,w,h = cv2.boundingRect(cnt)\n",
        "                ls_xmin.append(x)\n",
        "                ls_ymin.append(y)\n",
        "                ls_xmax.append(x + w)\n",
        "                ls_ymax.append(y + h)\n",
        "            xmin = min(ls_xmin)\n",
        "            ymin = min(ls_ymin)\n",
        "            xmax = max(ls_xmax)\n",
        "            ymax = max(ls_ymax)\n",
        "\n",
        "            roi = image[ymin:ymax,xmin:xmax]\n",
        "            resized_roi = cv2.resize(roi, (resize_size, resize_size),interpolation=cv2.INTER_AREA)\n",
        "            #image=affine_image(image)\n",
        "            #image= crop_resize(image)\n",
        "            #image = cv2.resize(image,(size,size),interpolation=cv2.INTER_AREA)\n",
        "            #image=resize_image(image,(64,64))\n",
        "            #image = cv2.resize(image,(size,size),interpolation=cv2.INTER_AREA)\n",
        "            #gaussian_3 = cv2.GaussianBlur(image, (5,5), cv2.BORDER_DEFAULT) #unblur\n",
        "            #image = cv2.addWeighted(image, 1.5, gaussian_3, -0.5, 0, image)\n",
        "            #kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]]) #filter\n",
        "            #image = cv2.filter2D(image, -1, kernel)\n",
        "            #ret,image = cv2.threshold(image, 128, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
        "            resized[df.index[i]] = resized_roi.reshape(-1)\n",
        "    resized = pd.DataFrame(resized).T\n",
        "    return resized\n",
        "\n",
        "def get_dummies(df):\n",
        "    cols = []\n",
        "    for col in df:\n",
        "        cols.append(pd.get_dummies(df[col].astype(str)))\n",
        "    return pd.concat(cols, axis=1)"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "K9G023ZpnEZ7",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "code",
      "source": [
        "IMG_SIZE=64\n",
        "N_CHANNELS=1"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "oND0qOAnnGki",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tensorflow.keras.backend.clear_session()\n",
        "inputs = Input(shape = (IMG_SIZE, IMG_SIZE, 1))\n",
        "model = Conv2D(filters=64, kernel_size=3, padding='SAME', activation='relu', input_shape=[IMG_SIZE, IMG_SIZE, 1],  use_bias=False)(inputs)\n",
        "model = BatchNormalization()(model)\n",
        "model = Activation(\"relu\")(model)\n",
        "model = MaxPool2D(pool_size=3, strides=2, padding=\"same\")(model)\n",
        "\n",
        "prev_filters = 64\n",
        "for filters in [64] * 3 + [128] * 4 + [256] * 6 + [512] * 3:\n",
        "  strides = 1 if filters == prev_filters else 2\n",
        "  model = ResidualUnit(filters, model, strides=strides).model\n",
        "  prev_filters = filters\n",
        "\n",
        "model = tensorflow.keras.layers.GlobalAvgPool2D()(model)\n",
        "# model = tensorflow.keras.layers.Flatten()(model)\n",
        "model = Dense(1024, activation = \"relu\")(model)\n",
        "model = Dropout(rate=0.3)(model)\n",
        "dense = Dense(512, activation = \"relu\")(model)\n",
        "\n",
        "head_root = tensorflow.keras.layers.Dense(168, activation = 'softmax', name = 'roots')(dense)\n",
        "head_vowel = tensorflow.keras.layers.Dense(11, activation = 'softmax', name = 'vowels')(dense)\n",
        "head_consonant = tensorflow.keras.layers.Dense(7, activation = 'softmax', name = 'consonants')(dense)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=[head_root, head_vowel, head_consonant])\n"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "8gDd0LaifrJn",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 64, 64, 1)]  0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 64, 64, 64)   576         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization (BatchNorma (None, 64, 64, 64)   256         conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "activation (Activation)         (None, 64, 64, 64)   0           batch_normalization[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 32, 32, 64)   0           activation[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 64)   36864       max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 64)   256         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Relu (TensorFlowOpL [(None, 32, 32, 64)] 0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 64)   36864       tf_op_layer_Relu[0][0]           \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 64)   256         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add (TensorFlowOpLa [(None, 32, 32, 64)] 0           batch_normalization_2[0][0]      \n",
            "                                                                 max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Relu_1 (TensorFlowO [(None, 32, 32, 64)] 0           tf_op_layer_add[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 64)   36864       tf_op_layer_Relu_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 64)   256         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Relu_2 (TensorFlowO [(None, 32, 32, 64)] 0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 64)   36864       tf_op_layer_Relu_2[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 64)   256         conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_1 (TensorFlowOp [(None, 32, 32, 64)] 0           batch_normalization_4[0][0]      \n",
            "                                                                 tf_op_layer_Relu_1[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Relu_3 (TensorFlowO [(None, 32, 32, 64)] 0           tf_op_layer_add_1[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 32, 32, 64)   36864       tf_op_layer_Relu_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 32, 32, 64)   256         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Relu_4 (TensorFlowO [(None, 32, 32, 64)] 0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 32, 32, 64)   36864       tf_op_layer_Relu_4[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 32, 32, 64)   256         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_2 (TensorFlowOp [(None, 32, 32, 64)] 0           batch_normalization_6[0][0]      \n",
            "                                                                 tf_op_layer_Relu_3[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Relu_5 (TensorFlowO [(None, 32, 32, 64)] 0           tf_op_layer_add_2[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 16, 16, 128)  73728       tf_op_layer_Relu_5[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 16, 16, 128)  512         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Relu_6 (TensorFlowO [(None, 16, 16, 128) 0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 16, 16, 128)  147456      tf_op_layer_Relu_6[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 16, 16, 128)  8192        tf_op_layer_Relu_5[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 16, 16, 128)  512         conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 16, 16, 128)  512         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_3 (TensorFlowOp [(None, 16, 16, 128) 0           batch_normalization_8[0][0]      \n",
            "                                                                 batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Relu_7 (TensorFlowO [(None, 16, 16, 128) 0           tf_op_layer_add_3[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 16, 16, 128)  147456      tf_op_layer_Relu_7[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 16, 16, 128)  512         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Relu_8 (TensorFlowO [(None, 16, 16, 128) 0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 16, 16, 128)  147456      tf_op_layer_Relu_8[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 16, 16, 128)  512         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_4 (TensorFlowOp [(None, 16, 16, 128) 0           batch_normalization_11[0][0]     \n",
            "                                                                 tf_op_layer_Relu_7[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Relu_9 (TensorFlowO [(None, 16, 16, 128) 0           tf_op_layer_add_4[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 16, 16, 128)  147456      tf_op_layer_Relu_9[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 16, 16, 128)  512         conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Relu_10 (TensorFlow [(None, 16, 16, 128) 0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 16, 16, 128)  147456      tf_op_layer_Relu_10[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 16, 16, 128)  512         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_5 (TensorFlowOp [(None, 16, 16, 128) 0           batch_normalization_13[0][0]     \n",
            "                                                                 tf_op_layer_Relu_9[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Relu_11 (TensorFlow [(None, 16, 16, 128) 0           tf_op_layer_add_5[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 16, 16, 128)  147456      tf_op_layer_Relu_11[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 16, 16, 128)  512         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Relu_12 (TensorFlow [(None, 16, 16, 128) 0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 16, 16, 128)  147456      tf_op_layer_Relu_12[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 16, 16, 128)  512         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_6 (TensorFlowOp [(None, 16, 16, 128) 0           batch_normalization_15[0][0]     \n",
            "                                                                 tf_op_layer_Relu_11[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Relu_13 (TensorFlow [(None, 16, 16, 128) 0           tf_op_layer_add_6[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 8, 8, 256)    294912      tf_op_layer_Relu_13[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 8, 8, 256)    1024        conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Relu_14 (TensorFlow [(None, 8, 8, 256)]  0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_17 (Conv2D)              (None, 8, 8, 256)    589824      tf_op_layer_Relu_14[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_18 (Conv2D)              (None, 8, 8, 256)    32768       tf_op_layer_Relu_13[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_17 (BatchNo (None, 8, 8, 256)    1024        conv2d_17[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_18 (BatchNo (None, 8, 8, 256)    1024        conv2d_18[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_7 (TensorFlowOp [(None, 8, 8, 256)]  0           batch_normalization_17[0][0]     \n",
            "                                                                 batch_normalization_18[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Relu_15 (TensorFlow [(None, 8, 8, 256)]  0           tf_op_layer_add_7[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_19 (Conv2D)              (None, 8, 8, 256)    589824      tf_op_layer_Relu_15[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_19 (BatchNo (None, 8, 8, 256)    1024        conv2d_19[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Relu_16 (TensorFlow [(None, 8, 8, 256)]  0           batch_normalization_19[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_20 (Conv2D)              (None, 8, 8, 256)    589824      tf_op_layer_Relu_16[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_20 (BatchNo (None, 8, 8, 256)    1024        conv2d_20[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_8 (TensorFlowOp [(None, 8, 8, 256)]  0           batch_normalization_20[0][0]     \n",
            "                                                                 tf_op_layer_Relu_15[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Relu_17 (TensorFlow [(None, 8, 8, 256)]  0           tf_op_layer_add_8[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_21 (Conv2D)              (None, 8, 8, 256)    589824      tf_op_layer_Relu_17[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_21 (BatchNo (None, 8, 8, 256)    1024        conv2d_21[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Relu_18 (TensorFlow [(None, 8, 8, 256)]  0           batch_normalization_21[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_22 (Conv2D)              (None, 8, 8, 256)    589824      tf_op_layer_Relu_18[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_22 (BatchNo (None, 8, 8, 256)    1024        conv2d_22[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_9 (TensorFlowOp [(None, 8, 8, 256)]  0           batch_normalization_22[0][0]     \n",
            "                                                                 tf_op_layer_Relu_17[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Relu_19 (TensorFlow [(None, 8, 8, 256)]  0           tf_op_layer_add_9[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_23 (Conv2D)              (None, 8, 8, 256)    589824      tf_op_layer_Relu_19[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_23 (BatchNo (None, 8, 8, 256)    1024        conv2d_23[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Relu_20 (TensorFlow [(None, 8, 8, 256)]  0           batch_normalization_23[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_24 (Conv2D)              (None, 8, 8, 256)    589824      tf_op_layer_Relu_20[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_24 (BatchNo (None, 8, 8, 256)    1024        conv2d_24[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_10 (TensorFlowO [(None, 8, 8, 256)]  0           batch_normalization_24[0][0]     \n",
            "                                                                 tf_op_layer_Relu_19[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Relu_21 (TensorFlow [(None, 8, 8, 256)]  0           tf_op_layer_add_10[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_25 (Conv2D)              (None, 8, 8, 256)    589824      tf_op_layer_Relu_21[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_25 (BatchNo (None, 8, 8, 256)    1024        conv2d_25[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Relu_22 (TensorFlow [(None, 8, 8, 256)]  0           batch_normalization_25[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_26 (Conv2D)              (None, 8, 8, 256)    589824      tf_op_layer_Relu_22[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_26 (BatchNo (None, 8, 8, 256)    1024        conv2d_26[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_11 (TensorFlowO [(None, 8, 8, 256)]  0           batch_normalization_26[0][0]     \n",
            "                                                                 tf_op_layer_Relu_21[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Relu_23 (TensorFlow [(None, 8, 8, 256)]  0           tf_op_layer_add_11[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_27 (Conv2D)              (None, 8, 8, 256)    589824      tf_op_layer_Relu_23[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_27 (BatchNo (None, 8, 8, 256)    1024        conv2d_27[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Relu_24 (TensorFlow [(None, 8, 8, 256)]  0           batch_normalization_27[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_28 (Conv2D)              (None, 8, 8, 256)    589824      tf_op_layer_Relu_24[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_28 (BatchNo (None, 8, 8, 256)    1024        conv2d_28[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_12 (TensorFlowO [(None, 8, 8, 256)]  0           batch_normalization_28[0][0]     \n",
            "                                                                 tf_op_layer_Relu_23[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Relu_25 (TensorFlow [(None, 8, 8, 256)]  0           tf_op_layer_add_12[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_29 (Conv2D)              (None, 4, 4, 512)    1179648     tf_op_layer_Relu_25[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_29 (BatchNo (None, 4, 4, 512)    2048        conv2d_29[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Relu_26 (TensorFlow [(None, 4, 4, 512)]  0           batch_normalization_29[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_30 (Conv2D)              (None, 4, 4, 512)    2359296     tf_op_layer_Relu_26[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_31 (Conv2D)              (None, 4, 4, 512)    131072      tf_op_layer_Relu_25[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_30 (BatchNo (None, 4, 4, 512)    2048        conv2d_30[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_31 (BatchNo (None, 4, 4, 512)    2048        conv2d_31[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_13 (TensorFlowO [(None, 4, 4, 512)]  0           batch_normalization_30[0][0]     \n",
            "                                                                 batch_normalization_31[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Relu_27 (TensorFlow [(None, 4, 4, 512)]  0           tf_op_layer_add_13[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_32 (Conv2D)              (None, 4, 4, 512)    2359296     tf_op_layer_Relu_27[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_32 (BatchNo (None, 4, 4, 512)    2048        conv2d_32[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Relu_28 (TensorFlow [(None, 4, 4, 512)]  0           batch_normalization_32[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_33 (Conv2D)              (None, 4, 4, 512)    2359296     tf_op_layer_Relu_28[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_33 (BatchNo (None, 4, 4, 512)    2048        conv2d_33[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_14 (TensorFlowO [(None, 4, 4, 512)]  0           batch_normalization_33[0][0]     \n",
            "                                                                 tf_op_layer_Relu_27[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Relu_29 (TensorFlow [(None, 4, 4, 512)]  0           tf_op_layer_add_14[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_34 (Conv2D)              (None, 4, 4, 512)    2359296     tf_op_layer_Relu_29[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_34 (BatchNo (None, 4, 4, 512)    2048        conv2d_34[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Relu_30 (TensorFlow [(None, 4, 4, 512)]  0           batch_normalization_34[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_35 (Conv2D)              (None, 4, 4, 512)    2359296     tf_op_layer_Relu_30[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_35 (BatchNo (None, 4, 4, 512)    2048        conv2d_35[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_add_15 (TensorFlowO [(None, 4, 4, 512)]  0           batch_normalization_35[0][0]     \n",
            "                                                                 tf_op_layer_Relu_29[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "tf_op_layer_Relu_31 (TensorFlow [(None, 4, 4, 512)]  0           tf_op_layer_add_15[0][0]         \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d (Globa (None, 512)          0           tf_op_layer_Relu_31[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 1024)         525312      global_average_pooling2d[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 1024)         0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 512)          524800      dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "roots (Dense)                   (None, 168)          86184       dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "vowels (Dense)                  (None, 11)           5643        dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "consonants (Dense)              (None, 7)            3591        dense_1[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 22,438,394\n",
            "Trainable params: 22,421,370\n",
            "Non-trainable params: 17,024\n",
            "__________________________________________________________________________________________________\n"
          ]
        }
      ],
      "execution_count": 29,
      "metadata": {
        "id": "RTLMbvIwnP7a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3e6a9e87-096e-4ca8-af9d-714a8d6de3e1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='nadam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "OLtstS3otZq-",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate_reduction_root = ReduceLROnPlateau(monitor='roots_acc', \n",
        "                                            patience=3, \n",
        "                                            verbose=1,\n",
        "                                            factor=0.5, \n",
        "                                            min_lr=0.00001)\n",
        "learning_rate_reduction_vowel = ReduceLROnPlateau(monitor='vowels_acc', \n",
        "                                            patience=3, \n",
        "                                            verbose=1,\n",
        "                                            factor=0.5, \n",
        "                                            min_lr=0.00001)\n",
        "learning_rate_reduction_consonant = ReduceLROnPlateau(monitor='consonants_acc', \n",
        "                                            patience=3, \n",
        "                                            verbose=1,\n",
        "                                            factor=0.5, \n",
        "                                            min_lr=0.00001)\n"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "gBLcbdgch4RI",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#decreased batch size\n",
        "batch_size = 256\n",
        "epochs = 30"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "jr4cS2fRtJNk",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiOutputDataGenerator(tensorflow.keras.preprocessing.image.ImageDataGenerator):\n",
        "\n",
        "    def flow(self,\n",
        "             x,\n",
        "             y=None,\n",
        "             batch_size=32,\n",
        "             shuffle=True,\n",
        "             sample_weight=None,\n",
        "             seed=None,\n",
        "             save_to_dir=None,\n",
        "             save_prefix='',\n",
        "             save_format='png',\n",
        "             subset=None):\n",
        "\n",
        "        targets = None\n",
        "        target_lengths = {}\n",
        "        ordered_outputs = []\n",
        "        for output, target in y.items():\n",
        "            if targets is None:\n",
        "                targets = target\n",
        "            else:\n",
        "                targets = np.concatenate((targets, target), axis=1)\n",
        "            target_lengths[output] = target.shape[1]\n",
        "            ordered_outputs.append(output)\n",
        "\n",
        "\n",
        "        for flowx, flowy in super().flow(x, targets, batch_size=batch_size,\n",
        "                                         shuffle=shuffle):\n",
        "            target_dict = {}\n",
        "            i = 0\n",
        "            for output in ordered_outputs:\n",
        "                target_length = target_lengths[output]\n",
        "                target_dict[output] = flowy[:, i: i + target_length]\n",
        "                i += target_length\n",
        "\n",
        "            yield flowx, target_dict"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "5RtCqDqitLxp",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "code",
      "source": [
        "histories = []"
      ],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "gK3_OgM8-yot",
        "colab_type": "code",
        "colab": {}
      }
    },
    {
      "cell_type": "code",
      "source": [
        "number_of_training_files = 1\n",
        "\n",
        "for i in range(number_of_training_files):\n",
        "    train_df = pd.merge(pd.read_parquet(f'drive/My Drive/Colab Notebooks/playground/train_image_data_{i}.parquet'), train_df_, on='image_id').drop(['image_id'], axis=1)\n",
        "    # Visualize few samples of current training dataset\n",
        "    # fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(16, 8))\n",
        "    # count=0\n",
        "    # for row in ax:\n",
        "    #     for col in row:\n",
        "    #         col.imshow(resize(train_df.drop(['grapheme_root', 'vowel_diacritic', 'consonant_diacritic'], axis=1).iloc[[count]], need_progress_bar=False).values.reshape(IMG_SIZE, IMG_SIZE))\n",
        "    #         count += 1\n",
        "    # plt.show()\n",
        "    \n",
        "    X_train = train_df.drop(['grapheme_root', 'vowel_diacritic', 'consonant_diacritic', 'grapheme'], axis=1)\n",
        "    X_train = resize(X_train)/255\n",
        "    \n",
        "    # CNN takes images in shape `(batch_size, h, w, channels)`, so reshape the images\n",
        "    X_train = X_train.values.reshape(-1, IMG_SIZE, IMG_SIZE, N_CHANNELS)\n",
        "    \n",
        "    Y_train_root = pd.get_dummies(train_df['grapheme_root']).values\n",
        "    Y_train_vowel = pd.get_dummies(train_df['vowel_diacritic']).values\n",
        "    Y_train_consonant = pd.get_dummies(train_df['consonant_diacritic']).values\n",
        "\n",
        "    print(f'Training images: {X_train.shape}')\n",
        "    print(f'Training labels root: {Y_train_root.shape}')\n",
        "    print(f'Training labels vowel: {Y_train_vowel.shape}')\n",
        "    print(f'Training labels consonants: {Y_train_consonant.shape}')\n",
        "\n",
        "    # Divide the data into training and validation set\n",
        "    x_train, x_test, y_train_root, y_test_root, y_train_vowel, y_test_vowel, y_train_consonant, y_test_consonant = train_test_split(X_train, Y_train_root, Y_train_vowel, Y_train_consonant, test_size=0.08, random_state=666)\n",
        "    del train_df\n",
        "    del X_train\n",
        "    del Y_train_root, Y_train_vowel, Y_train_consonant\n",
        "\n",
        "    # Data augmentation for creating more training data\n",
        "    datagen = MultiOutputDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=8,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = 0.15, # Randomly zoom image \n",
        "        width_shift_range=0.15,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.15,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=False,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "\n",
        "\n",
        "    # This will just calculate parameters required to augment the given data. This won't perform any augmentations\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # Fit the model\n",
        "    history = model.fit_generator(datagen.flow(x_train, {'roots': y_train_root, 'vowels': y_train_vowel, 'consonants': y_train_consonant}, batch_size=batch_size),\n",
        "                              epochs = epochs, validation_data = (x_test, [y_test_root, y_test_vowel, y_test_consonant]), \n",
        "                              steps_per_epoch=x_train.shape[0] // batch_size, \n",
        "                              callbacks=[learning_rate_reduction_root, learning_rate_reduction_vowel, learning_rate_reduction_consonant])\n",
        "\n",
        "    histories.append(history)\n",
        "    \n",
        "    # Delete to reduce memory usage\n",
        "    del x_train\n",
        "    del x_test\n",
        "    del y_train_root\n",
        "    del y_test_root\n",
        "    del y_train_vowel\n",
        "    del y_test_vowel\n",
        "    del y_train_consonant\n",
        "    del y_test_consonant\n",
        "    gc.collect()"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db8f389836774c6a815e1eb8369b5796",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=50210), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training images: (50210, 64, 64, 1)\n",
            "Training labels root: (50210, 168)\n",
            "Training labels vowel: (50210, 11)\n",
            "Training labels consonants: (50210, 7)\n",
            "Epoch 1/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.6428 - roots_loss: 0.3584 - vowels_loss: 0.1631 - consonants_loss: 0.1212 - roots_acc: 0.9040 - vowels_acc: 0.9547 - consonants_acc: 0.9644Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 273us/sample - loss: 1.2488 - roots_loss: 0.5152 - vowels_loss: 0.5129 - consonants_loss: 0.2663 - roots_acc: 0.8586 - vowels_acc: 0.8603 - consonants_acc: 0.9106\n",
            "180/180 [==============================] - 46s 255ms/step - loss: 0.6434 - roots_loss: 0.3581 - vowels_loss: 0.1634 - consonants_loss: 0.1220 - roots_acc: 0.9040 - vowels_acc: 0.9546 - consonants_acc: 0.9642 - val_loss: 1.2944 - val_roots_loss: 0.5152 - val_vowels_loss: 0.5129 - val_consonants_loss: 0.2663 - val_roots_acc: 0.8586 - val_vowels_acc: 0.8603 - val_consonants_acc: 0.9106\n",
            "Epoch 2/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.5340 - roots_loss: 0.2885 - vowels_loss: 0.1458 - consonants_loss: 0.0998 - roots_acc: 0.9187 - vowels_acc: 0.9572 - consonants_acc: 0.9706Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 275us/sample - loss: 0.3096 - roots_loss: 0.2154 - vowels_loss: 0.0770 - consonants_loss: 0.0718 - roots_acc: 0.9403 - vowels_acc: 0.9801 - consonants_acc: 0.9798\n",
            "180/180 [==============================] - 45s 251ms/step - loss: 0.5341 - roots_loss: 0.2885 - vowels_loss: 0.1459 - consonants_loss: 0.0997 - roots_acc: 0.9186 - vowels_acc: 0.9571 - consonants_acc: 0.9707 - val_loss: 0.3641 - val_roots_loss: 0.2154 - val_vowels_loss: 0.0770 - val_consonants_loss: 0.0718 - val_roots_acc: 0.9403 - val_vowels_acc: 0.9801 - val_consonants_acc: 0.9798\n",
            "Epoch 3/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.4841 - roots_loss: 0.2560 - vowels_loss: 0.1348 - consonants_loss: 0.0934 - roots_acc: 0.9244 - vowels_acc: 0.9607 - consonants_acc: 0.9718Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 273us/sample - loss: 0.4542 - roots_loss: 0.2559 - vowels_loss: 0.1481 - consonants_loss: 0.0850 - roots_acc: 0.9253 - vowels_acc: 0.9572 - consonants_acc: 0.9729\n",
            "180/180 [==============================] - 45s 251ms/step - loss: 0.4844 - roots_loss: 0.2560 - vowels_loss: 0.1350 - consonants_loss: 0.0934 - roots_acc: 0.9244 - vowels_acc: 0.9606 - consonants_acc: 0.9717 - val_loss: 0.4890 - val_roots_loss: 0.2559 - val_vowels_loss: 0.1481 - val_consonants_loss: 0.0850 - val_roots_acc: 0.9253 - val_vowels_acc: 0.9572 - val_consonants_acc: 0.9729\n",
            "Epoch 4/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.4558 - roots_loss: 0.2317 - vowels_loss: 0.1330 - consonants_loss: 0.0911 - roots_acc: 0.9321 - vowels_acc: 0.9602 - consonants_acc: 0.9729Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 274us/sample - loss: 1.9548 - roots_loss: 1.1809 - vowels_loss: 0.4734 - consonants_loss: 0.2544 - roots_acc: 0.7160 - vowels_acc: 0.8392 - consonants_acc: 0.9149\n",
            "180/180 [==============================] - 45s 251ms/step - loss: 0.4565 - roots_loss: 0.2319 - vowels_loss: 0.1333 - consonants_loss: 0.0913 - roots_acc: 0.9320 - vowels_acc: 0.9602 - consonants_acc: 0.9728 - val_loss: 1.9086 - val_roots_loss: 1.1809 - val_vowels_loss: 0.4734 - val_consonants_loss: 0.2544 - val_roots_acc: 0.7160 - val_vowels_acc: 0.8392 - val_consonants_acc: 0.9149\n",
            "Epoch 5/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.4192 - roots_loss: 0.2169 - vowels_loss: 0.1210 - consonants_loss: 0.0812 - roots_acc: 0.9351 - vowels_acc: 0.9653 - consonants_acc: 0.9753Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 272us/sample - loss: 1.2867 - roots_loss: 0.5979 - vowels_loss: 0.4838 - consonants_loss: 0.2094 - roots_acc: 0.8464 - vowels_acc: 0.8519 - consonants_acc: 0.9276\n",
            "180/180 [==============================] - 45s 252ms/step - loss: 0.4189 - roots_loss: 0.2166 - vowels_loss: 0.1210 - consonants_loss: 0.0813 - roots_acc: 0.9353 - vowels_acc: 0.9654 - consonants_acc: 0.9753 - val_loss: 1.2911 - val_roots_loss: 0.5979 - val_vowels_loss: 0.4838 - val_consonants_loss: 0.2094 - val_roots_acc: 0.8464 - val_vowels_acc: 0.8519 - val_consonants_acc: 0.9276\n",
            "Epoch 6/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.4032 - roots_loss: 0.2024 - vowels_loss: 0.1223 - consonants_loss: 0.0786 - roots_acc: 0.9401 - vowels_acc: 0.9654 - consonants_acc: 0.9761Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 272us/sample - loss: 0.3359 - roots_loss: 0.2277 - vowels_loss: 0.0853 - consonants_loss: 0.0711 - roots_acc: 0.9395 - vowels_acc: 0.9766 - consonants_acc: 0.9801\n",
            "180/180 [==============================] - 45s 251ms/step - loss: 0.4029 - roots_loss: 0.2022 - vowels_loss: 0.1223 - consonants_loss: 0.0783 - roots_acc: 0.9401 - vowels_acc: 0.9654 - consonants_acc: 0.9762 - val_loss: 0.3841 - val_roots_loss: 0.2277 - val_vowels_loss: 0.0853 - val_consonants_loss: 0.0711 - val_roots_acc: 0.9395 - val_vowels_acc: 0.9766 - val_consonants_acc: 0.9801\n",
            "Epoch 7/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.3861 - roots_loss: 0.1966 - vowels_loss: 0.1124 - consonants_loss: 0.0770 - roots_acc: 0.9402 - vowels_acc: 0.9672 - consonants_acc: 0.9768Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 272us/sample - loss: 0.7113 - roots_loss: 0.2537 - vowels_loss: 0.3481 - consonants_loss: 0.0854 - roots_acc: 0.9310 - vowels_acc: 0.9034 - consonants_acc: 0.9744\n",
            "180/180 [==============================] - 45s 252ms/step - loss: 0.3863 - roots_loss: 0.1969 - vowels_loss: 0.1123 - consonants_loss: 0.0771 - roots_acc: 0.9401 - vowels_acc: 0.9673 - consonants_acc: 0.9768 - val_loss: 0.6872 - val_roots_loss: 0.2537 - val_vowels_loss: 0.3481 - val_consonants_loss: 0.0854 - val_roots_acc: 0.9310 - val_vowels_acc: 0.9034 - val_consonants_acc: 0.9744\n",
            "Epoch 8/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.3737 - roots_loss: 0.1884 - vowels_loss: 0.1130 - consonants_loss: 0.0724 - roots_acc: 0.9431 - vowels_acc: 0.9667 - consonants_acc: 0.9782Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 273us/sample - loss: 0.3829 - roots_loss: 0.2224 - vowels_loss: 0.0774 - consonants_loss: 0.0729 - roots_acc: 0.9403 - vowels_acc: 0.9818 - consonants_acc: 0.9798\n",
            "180/180 [==============================] - 45s 251ms/step - loss: 0.3736 - roots_loss: 0.1885 - vowels_loss: 0.1127 - consonants_loss: 0.0724 - roots_acc: 0.9430 - vowels_acc: 0.9667 - consonants_acc: 0.9782 - val_loss: 0.3727 - val_roots_loss: 0.2224 - val_vowels_loss: 0.0774 - val_consonants_loss: 0.0729 - val_roots_acc: 0.9403 - val_vowels_acc: 0.9818 - val_consonants_acc: 0.9798\n",
            "Epoch 9/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.3574 - roots_loss: 0.1733 - vowels_loss: 0.1105 - consonants_loss: 0.0736 - roots_acc: 0.9480 - vowels_acc: 0.9678 - consonants_acc: 0.9780Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 273us/sample - loss: 0.4382 - roots_loss: 0.2312 - vowels_loss: 0.1264 - consonants_loss: 0.0745 - roots_acc: 0.9383 - vowels_acc: 0.9624 - consonants_acc: 0.9796\n",
            "180/180 [==============================] - 45s 252ms/step - loss: 0.3575 - roots_loss: 0.1733 - vowels_loss: 0.1104 - consonants_loss: 0.0738 - roots_acc: 0.9480 - vowels_acc: 0.9678 - consonants_acc: 0.9780 - val_loss: 0.4321 - val_roots_loss: 0.2312 - val_vowels_loss: 0.1264 - val_consonants_loss: 0.0745 - val_roots_acc: 0.9383 - val_vowels_acc: 0.9624 - val_consonants_acc: 0.9796\n",
            "Epoch 10/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.3449 - roots_loss: 0.1718 - vowels_loss: 0.1046 - consonants_loss: 0.0685 - roots_acc: 0.9472 - vowels_acc: 0.9701 - consonants_acc: 0.9791Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 280us/sample - loss: 0.9582 - roots_loss: 0.3391 - vowels_loss: 0.3779 - consonants_loss: 0.2207 - roots_acc: 0.9037 - vowels_acc: 0.8855 - consonants_acc: 0.9313\n",
            "180/180 [==============================] - 45s 251ms/step - loss: 0.3445 - roots_loss: 0.1714 - vowels_loss: 0.1046 - consonants_loss: 0.0685 - roots_acc: 0.9473 - vowels_acc: 0.9702 - consonants_acc: 0.9791 - val_loss: 0.9377 - val_roots_loss: 0.3391 - val_vowels_loss: 0.3779 - val_consonants_loss: 0.2207 - val_roots_acc: 0.9037 - val_vowels_acc: 0.8855 - val_consonants_acc: 0.9313\n",
            "Epoch 11/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.3331 - roots_loss: 0.1575 - vowels_loss: 0.1041 - consonants_loss: 0.0715 - roots_acc: 0.9513 - vowels_acc: 0.9693 - consonants_acc: 0.9789Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 273us/sample - loss: 0.4180 - roots_loss: 0.2237 - vowels_loss: 0.1469 - consonants_loss: 0.0733 - roots_acc: 0.9442 - vowels_acc: 0.9597 - consonants_acc: 0.9783\n",
            "180/180 [==============================] - 45s 251ms/step - loss: 0.3333 - roots_loss: 0.1575 - vowels_loss: 0.1042 - consonants_loss: 0.0716 - roots_acc: 0.9513 - vowels_acc: 0.9693 - consonants_acc: 0.9789 - val_loss: 0.4439 - val_roots_loss: 0.2237 - val_vowels_loss: 0.1469 - val_consonants_loss: 0.0733 - val_roots_acc: 0.9442 - val_vowels_acc: 0.9597 - val_consonants_acc: 0.9783\n",
            "Epoch 12/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.3237 - roots_loss: 0.1544 - vowels_loss: 0.1012 - consonants_loss: 0.0680 - roots_acc: 0.9529 - vowels_acc: 0.9699 - consonants_acc: 0.9797Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 275us/sample - loss: 0.4712 - roots_loss: 0.2404 - vowels_loss: 0.1056 - consonants_loss: 0.0927 - roots_acc: 0.9363 - vowels_acc: 0.9724 - consonants_acc: 0.9746\n",
            "180/180 [==============================] - 45s 251ms/step - loss: 0.3235 - roots_loss: 0.1544 - vowels_loss: 0.1012 - consonants_loss: 0.0679 - roots_acc: 0.9529 - vowels_acc: 0.9699 - consonants_acc: 0.9798 - val_loss: 0.4386 - val_roots_loss: 0.2404 - val_vowels_loss: 0.1056 - val_consonants_loss: 0.0927 - val_roots_acc: 0.9363 - val_vowels_acc: 0.9724 - val_consonants_acc: 0.9746\n",
            "Epoch 13/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.3151 - roots_loss: 0.1508 - vowels_loss: 0.1007 - consonants_loss: 0.0637 - roots_acc: 0.9537 - vowels_acc: 0.9703 - consonants_acc: 0.9800Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 273us/sample - loss: 0.4051 - roots_loss: 0.2413 - vowels_loss: 0.0740 - consonants_loss: 0.0748 - roots_acc: 0.9390 - vowels_acc: 0.9816 - consonants_acc: 0.9801\n",
            "180/180 [==============================] - 45s 251ms/step - loss: 0.3157 - roots_loss: 0.1513 - vowels_loss: 0.1005 - consonants_loss: 0.0638 - roots_acc: 0.9535 - vowels_acc: 0.9704 - consonants_acc: 0.9800 - val_loss: 0.3900 - val_roots_loss: 0.2413 - val_vowels_loss: 0.0740 - val_consonants_loss: 0.0748 - val_roots_acc: 0.9390 - val_vowels_acc: 0.9816 - val_consonants_acc: 0.9801\n",
            "Epoch 14/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.3103 - roots_loss: 0.1490 - vowels_loss: 0.0992 - consonants_loss: 0.0622 - roots_acc: 0.9551 - vowels_acc: 0.9717 - consonants_acc: 0.9811Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 277us/sample - loss: 0.4015 - roots_loss: 0.2454 - vowels_loss: 0.0865 - consonants_loss: 0.0813 - roots_acc: 0.9355 - vowels_acc: 0.9801 - consonants_acc: 0.9778\n",
            "180/180 [==============================] - 45s 252ms/step - loss: 0.3099 - roots_loss: 0.1488 - vowels_loss: 0.0991 - consonants_loss: 0.0620 - roots_acc: 0.9551 - vowels_acc: 0.9717 - consonants_acc: 0.9812 - val_loss: 0.4132 - val_roots_loss: 0.2454 - val_vowels_loss: 0.0865 - val_consonants_loss: 0.0813 - val_roots_acc: 0.9355 - val_vowels_acc: 0.9801 - val_consonants_acc: 0.9778\n",
            "Epoch 15/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.3033 - roots_loss: 0.1465 - vowels_loss: 0.0940 - consonants_loss: 0.0628 - roots_acc: 0.9545 - vowels_acc: 0.9722 - consonants_acc: 0.9812Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 274us/sample - loss: 0.3902 - roots_loss: 0.2298 - vowels_loss: 0.0849 - consonants_loss: 0.0747 - roots_acc: 0.9452 - vowels_acc: 0.9796 - consonants_acc: 0.9818\n",
            "180/180 [==============================] - 45s 251ms/step - loss: 0.3033 - roots_loss: 0.1466 - vowels_loss: 0.0940 - consonants_loss: 0.0627 - roots_acc: 0.9545 - vowels_acc: 0.9723 - consonants_acc: 0.9813 - val_loss: 0.3895 - val_roots_loss: 0.2298 - val_vowels_loss: 0.0849 - val_consonants_loss: 0.0747 - val_roots_acc: 0.9452 - val_vowels_acc: 0.9796 - val_consonants_acc: 0.9818\n",
            "Epoch 16/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2936 - roots_loss: 0.1374 - vowels_loss: 0.0966 - consonants_loss: 0.0597 - roots_acc: 0.9572 - vowels_acc: 0.9709 - consonants_acc: 0.9819Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 279us/sample - loss: 0.5006 - roots_loss: 0.2629 - vowels_loss: 0.1395 - consonants_loss: 0.0878 - roots_acc: 0.9318 - vowels_acc: 0.9634 - consonants_acc: 0.9781\n",
            "180/180 [==============================] - 45s 251ms/step - loss: 0.2935 - roots_loss: 0.1374 - vowels_loss: 0.0965 - consonants_loss: 0.0597 - roots_acc: 0.9572 - vowels_acc: 0.9710 - consonants_acc: 0.9819 - val_loss: 0.4901 - val_roots_loss: 0.2629 - val_vowels_loss: 0.1395 - val_consonants_loss: 0.0878 - val_roots_acc: 0.9318 - val_vowels_acc: 0.9634 - val_consonants_acc: 0.9781\n",
            "Epoch 17/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2934 - roots_loss: 0.1365 - vowels_loss: 0.0984 - consonants_loss: 0.0584 - roots_acc: 0.9569 - vowels_acc: 0.9718 - consonants_acc: 0.9826Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 277us/sample - loss: 0.3923 - roots_loss: 0.2214 - vowels_loss: 0.0836 - consonants_loss: 0.0790 - roots_acc: 0.9442 - vowels_acc: 0.9788 - consonants_acc: 0.9796\n",
            "180/180 [==============================] - 45s 251ms/step - loss: 0.2932 - roots_loss: 0.1364 - vowels_loss: 0.0984 - consonants_loss: 0.0584 - roots_acc: 0.9570 - vowels_acc: 0.9719 - consonants_acc: 0.9827 - val_loss: 0.3839 - val_roots_loss: 0.2214 - val_vowels_loss: 0.0836 - val_consonants_loss: 0.0790 - val_roots_acc: 0.9442 - val_vowels_acc: 0.9788 - val_consonants_acc: 0.9796\n",
            "Epoch 18/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2820 - roots_loss: 0.1316 - vowels_loss: 0.0923 - consonants_loss: 0.0581 - roots_acc: 0.9590 - vowels_acc: 0.9723 - consonants_acc: 0.9827Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 271us/sample - loss: 0.4285 - roots_loss: 0.2584 - vowels_loss: 0.0857 - consonants_loss: 0.0730 - roots_acc: 0.9330 - vowels_acc: 0.9798 - consonants_acc: 0.9813\n",
            "\n",
            "Epoch 00018: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
            "180/180 [==============================] - 45s 251ms/step - loss: 0.2824 - roots_loss: 0.1318 - vowels_loss: 0.0922 - consonants_loss: 0.0584 - roots_acc: 0.9590 - vowels_acc: 0.9724 - consonants_acc: 0.9827 - val_loss: 0.4171 - val_roots_loss: 0.2584 - val_vowels_loss: 0.0857 - val_consonants_loss: 0.0730 - val_roots_acc: 0.9330 - val_vowels_acc: 0.9798 - val_consonants_acc: 0.9813\n",
            "Epoch 19/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2258 - roots_loss: 0.0971 - vowels_loss: 0.0809 - consonants_loss: 0.0478 - roots_acc: 0.9692 - vowels_acc: 0.9755 - consonants_acc: 0.9857Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 277us/sample - loss: 0.3243 - roots_loss: 0.1945 - vowels_loss: 0.0767 - consonants_loss: 0.0695 - roots_acc: 0.9525 - vowels_acc: 0.9818 - consonants_acc: 0.9838\n",
            "180/180 [==============================] - 45s 252ms/step - loss: 0.2257 - roots_loss: 0.0970 - vowels_loss: 0.0809 - consonants_loss: 0.0478 - roots_acc: 0.9692 - vowels_acc: 0.9755 - consonants_acc: 0.9857 - val_loss: 0.3407 - val_roots_loss: 0.1945 - val_vowels_loss: 0.0767 - val_consonants_loss: 0.0695 - val_roots_acc: 0.9525 - val_vowels_acc: 0.9818 - val_consonants_acc: 0.9838\n",
            "Epoch 20/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2047 - roots_loss: 0.0855 - vowels_loss: 0.0742 - consonants_loss: 0.0450 - roots_acc: 0.9732 - vowels_acc: 0.9768 - consonants_acc: 0.9865Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 278us/sample - loss: 0.6625 - roots_loss: 0.2457 - vowels_loss: 0.3089 - consonants_loss: 0.0907 - roots_acc: 0.9338 - vowels_acc: 0.9221 - consonants_acc: 0.9719\n",
            "180/180 [==============================] - 45s 253ms/step - loss: 0.2047 - roots_loss: 0.0854 - vowels_loss: 0.0740 - consonants_loss: 0.0453 - roots_acc: 0.9732 - vowels_acc: 0.9769 - consonants_acc: 0.9865 - val_loss: 0.6454 - val_roots_loss: 0.2457 - val_vowels_loss: 0.3089 - val_consonants_loss: 0.0907 - val_roots_acc: 0.9338 - val_vowels_acc: 0.9221 - val_consonants_acc: 0.9719\n",
            "Epoch 21/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2068 - roots_loss: 0.0864 - vowels_loss: 0.0775 - consonants_loss: 0.0429 - roots_acc: 0.9726 - vowels_acc: 0.9767 - consonants_acc: 0.9864Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 279us/sample - loss: 0.3229 - roots_loss: 0.2081 - vowels_loss: 0.0707 - consonants_loss: 0.0610 - roots_acc: 0.9507 - vowels_acc: 0.9831 - consonants_acc: 0.9838\n",
            "180/180 [==============================] - 45s 253ms/step - loss: 0.2068 - roots_loss: 0.0864 - vowels_loss: 0.0774 - consonants_loss: 0.0430 - roots_acc: 0.9726 - vowels_acc: 0.9767 - consonants_acc: 0.9864 - val_loss: 0.3398 - val_roots_loss: 0.2081 - val_vowels_loss: 0.0707 - val_consonants_loss: 0.0610 - val_roots_acc: 0.9507 - val_vowels_acc: 0.9831 - val_consonants_acc: 0.9838\n",
            "Epoch 22/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2016 - roots_loss: 0.0861 - vowels_loss: 0.0724 - consonants_loss: 0.0430 - roots_acc: 0.9724 - vowels_acc: 0.9778 - consonants_acc: 0.9872Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 276us/sample - loss: 1.6711 - roots_loss: 0.3923 - vowels_loss: 0.8751 - consonants_loss: 0.3283 - roots_acc: 0.8992 - vowels_acc: 0.7842 - consonants_acc: 0.9166\n",
            "180/180 [==============================] - 46s 253ms/step - loss: 0.2016 - roots_loss: 0.0861 - vowels_loss: 0.0725 - consonants_loss: 0.0430 - roots_acc: 0.9724 - vowels_acc: 0.9779 - consonants_acc: 0.9872 - val_loss: 1.5957 - val_roots_loss: 0.3923 - val_vowels_loss: 0.8751 - val_consonants_loss: 0.3283 - val_roots_acc: 0.8992 - val_vowels_acc: 0.7842 - val_consonants_acc: 0.9166\n",
            "Epoch 23/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.1909 - roots_loss: 0.0801 - vowels_loss: 0.0701 - consonants_loss: 0.0407 - roots_acc: 0.9745 - vowels_acc: 0.9791 - consonants_acc: 0.9879Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 274us/sample - loss: 0.3939 - roots_loss: 0.2281 - vowels_loss: 0.0903 - consonants_loss: 0.0751 - roots_acc: 0.9437 - vowels_acc: 0.9783 - consonants_acc: 0.9808\n",
            "180/180 [==============================] - 45s 252ms/step - loss: 0.1909 - roots_loss: 0.0801 - vowels_loss: 0.0702 - consonants_loss: 0.0406 - roots_acc: 0.9746 - vowels_acc: 0.9791 - consonants_acc: 0.9879 - val_loss: 0.3935 - val_roots_loss: 0.2281 - val_vowels_loss: 0.0903 - val_consonants_loss: 0.0751 - val_roots_acc: 0.9437 - val_vowels_acc: 0.9783 - val_consonants_acc: 0.9808\n",
            "Epoch 24/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.1857 - roots_loss: 0.0796 - vowels_loss: 0.0673 - consonants_loss: 0.0388 - roots_acc: 0.9759 - vowels_acc: 0.9794 - consonants_acc: 0.9880Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 280us/sample - loss: 0.3585 - roots_loss: 0.2107 - vowels_loss: 0.0959 - consonants_loss: 0.0698 - roots_acc: 0.9465 - vowels_acc: 0.9764 - consonants_acc: 0.9816\n",
            "180/180 [==============================] - 46s 253ms/step - loss: 0.1862 - roots_loss: 0.0799 - vowels_loss: 0.0674 - consonants_loss: 0.0389 - roots_acc: 0.9758 - vowels_acc: 0.9795 - consonants_acc: 0.9880 - val_loss: 0.3764 - val_roots_loss: 0.2107 - val_vowels_loss: 0.0959 - val_consonants_loss: 0.0698 - val_roots_acc: 0.9465 - val_vowels_acc: 0.9764 - val_consonants_acc: 0.9816\n",
            "Epoch 25/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.1855 - roots_loss: 0.0785 - vowels_loss: 0.0661 - consonants_loss: 0.0410 - roots_acc: 0.9755 - vowels_acc: 0.9800 - consonants_acc: 0.9873Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 279us/sample - loss: 0.3210 - roots_loss: 0.2133 - vowels_loss: 0.0867 - consonants_loss: 0.0646 - roots_acc: 0.9497 - vowels_acc: 0.9788 - consonants_acc: 0.9828\n",
            "180/180 [==============================] - 46s 254ms/step - loss: 0.1853 - roots_loss: 0.0784 - vowels_loss: 0.0659 - consonants_loss: 0.0410 - roots_acc: 0.9755 - vowels_acc: 0.9800 - consonants_acc: 0.9872 - val_loss: 0.3646 - val_roots_loss: 0.2133 - val_vowels_loss: 0.0867 - val_consonants_loss: 0.0646 - val_roots_acc: 0.9497 - val_vowels_acc: 0.9788 - val_consonants_acc: 0.9828\n",
            "Epoch 26/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.1844 - roots_loss: 0.0744 - vowels_loss: 0.0679 - consonants_loss: 0.0421 - roots_acc: 0.9766 - vowels_acc: 0.9792 - consonants_acc: 0.9867Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 285us/sample - loss: 0.3851 - roots_loss: 0.2232 - vowels_loss: 0.1208 - consonants_loss: 0.0758 - roots_acc: 0.9467 - vowels_acc: 0.9659 - consonants_acc: 0.9818\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
            "180/180 [==============================] - 46s 254ms/step - loss: 0.1839 - roots_loss: 0.0742 - vowels_loss: 0.0678 - consonants_loss: 0.0420 - roots_acc: 0.9766 - vowels_acc: 0.9793 - consonants_acc: 0.9868 - val_loss: 0.4198 - val_roots_loss: 0.2232 - val_vowels_loss: 0.1208 - val_consonants_loss: 0.0758 - val_roots_acc: 0.9467 - val_vowels_acc: 0.9659 - val_consonants_acc: 0.9818\n",
            "Epoch 27/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.1631 - roots_loss: 0.0666 - vowels_loss: 0.0618 - consonants_loss: 0.0346 - roots_acc: 0.9797 - vowels_acc: 0.9810 - consonants_acc: 0.9896Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 279us/sample - loss: 0.3053 - roots_loss: 0.2013 - vowels_loss: 0.0724 - consonants_loss: 0.0651 - roots_acc: 0.9529 - vowels_acc: 0.9833 - consonants_acc: 0.9851\n",
            "180/180 [==============================] - 46s 254ms/step - loss: 0.1632 - roots_loss: 0.0669 - vowels_loss: 0.0619 - consonants_loss: 0.0345 - roots_acc: 0.9796 - vowels_acc: 0.9810 - consonants_acc: 0.9896 - val_loss: 0.3389 - val_roots_loss: 0.2013 - val_vowels_loss: 0.0724 - val_consonants_loss: 0.0651 - val_roots_acc: 0.9529 - val_vowels_acc: 0.9833 - val_consonants_acc: 0.9851\n",
            "Epoch 28/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.1471 - roots_loss: 0.0589 - vowels_loss: 0.0569 - consonants_loss: 0.0313 - roots_acc: 0.9815 - vowels_acc: 0.9825 - consonants_acc: 0.9903Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 277us/sample - loss: 0.3564 - roots_loss: 0.2098 - vowels_loss: 0.0766 - consonants_loss: 0.0721 - roots_acc: 0.9510 - vowels_acc: 0.9838 - consonants_acc: 0.9826\n",
            "180/180 [==============================] - 46s 254ms/step - loss: 0.1470 - roots_loss: 0.0588 - vowels_loss: 0.0568 - consonants_loss: 0.0314 - roots_acc: 0.9815 - vowels_acc: 0.9826 - consonants_acc: 0.9903 - val_loss: 0.3585 - val_roots_loss: 0.2098 - val_vowels_loss: 0.0766 - val_consonants_loss: 0.0721 - val_roots_acc: 0.9510 - val_vowels_acc: 0.9838 - val_consonants_acc: 0.9826\n",
            "Epoch 29/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.1515 - roots_loss: 0.0600 - vowels_loss: 0.0593 - consonants_loss: 0.0322 - roots_acc: 0.9809 - vowels_acc: 0.9821 - consonants_acc: 0.9902Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 272us/sample - loss: 0.3446 - roots_loss: 0.2132 - vowels_loss: 0.0795 - consonants_loss: 0.0686 - roots_acc: 0.9525 - vowels_acc: 0.9833 - consonants_acc: 0.9838\n",
            "180/180 [==============================] - 45s 251ms/step - loss: 0.1515 - roots_loss: 0.0602 - vowels_loss: 0.0592 - consonants_loss: 0.0322 - roots_acc: 0.9809 - vowels_acc: 0.9821 - consonants_acc: 0.9902 - val_loss: 0.3614 - val_roots_loss: 0.2132 - val_vowels_loss: 0.0795 - val_consonants_loss: 0.0686 - val_roots_acc: 0.9525 - val_vowels_acc: 0.9833 - val_consonants_acc: 0.9838\n",
            "Epoch 30/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.1464 - roots_loss: 0.0574 - vowels_loss: 0.0572 - consonants_loss: 0.0318 - roots_acc: 0.9818 - vowels_acc: 0.9827 - consonants_acc: 0.9904Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 272us/sample - loss: 0.3457 - roots_loss: 0.2100 - vowels_loss: 0.0790 - consonants_loss: 0.0692 - roots_acc: 0.9515 - vowels_acc: 0.9833 - consonants_acc: 0.9848\n",
            "180/180 [==============================] - 45s 250ms/step - loss: 0.1467 - roots_loss: 0.0575 - vowels_loss: 0.0573 - consonants_loss: 0.0319 - roots_acc: 0.9818 - vowels_acc: 0.9827 - consonants_acc: 0.9904 - val_loss: 0.3581 - val_roots_loss: 0.2100 - val_vowels_loss: 0.0790 - val_consonants_loss: 0.0692 - val_roots_acc: 0.9515 - val_vowels_acc: 0.9833 - val_consonants_acc: 0.9848\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "271da1772d014b7eb0b661e0f554858f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=50210), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training images: (50210, 64, 64, 1)\n",
            "Training labels root: (50210, 168)\n",
            "Training labels vowel: (50210, 11)\n",
            "Training labels consonants: (50210, 7)\n",
            "Epoch 1/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.4907 - roots_loss: 0.2750 - vowels_loss: 0.1237 - consonants_loss: 0.0920 - roots_acc: 0.9269 - vowels_acc: 0.9660 - consonants_acc: 0.9742Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 274us/sample - loss: 0.3945 - roots_loss: 0.1969 - vowels_loss: 0.0613 - consonants_loss: 0.0644 - roots_acc: 0.9507 - vowels_acc: 0.9861 - consonants_acc: 0.9823\n",
            "180/180 [==============================] - 46s 253ms/step - loss: 0.4902 - roots_loss: 0.2748 - vowels_loss: 0.1234 - consonants_loss: 0.0920 - roots_acc: 0.9269 - vowels_acc: 0.9661 - consonants_acc: 0.9742 - val_loss: 0.3226 - val_roots_loss: 0.1969 - val_vowels_loss: 0.0613 - val_consonants_loss: 0.0644 - val_roots_acc: 0.9507 - val_vowels_acc: 0.9861 - val_consonants_acc: 0.9823\n",
            "Epoch 2/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.4108 - roots_loss: 0.2231 - vowels_loss: 0.1102 - consonants_loss: 0.0776 - roots_acc: 0.9365 - vowels_acc: 0.9690 - consonants_acc: 0.9772Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 272us/sample - loss: 0.4022 - roots_loss: 0.2046 - vowels_loss: 0.0770 - consonants_loss: 0.0700 - roots_acc: 0.9487 - vowels_acc: 0.9818 - consonants_acc: 0.9801\n",
            "180/180 [==============================] - 45s 251ms/step - loss: 0.4114 - roots_loss: 0.2233 - vowels_loss: 0.1104 - consonants_loss: 0.0777 - roots_acc: 0.9365 - vowels_acc: 0.9689 - consonants_acc: 0.9772 - val_loss: 0.3516 - val_roots_loss: 0.2046 - val_vowels_loss: 0.0770 - val_consonants_loss: 0.0700 - val_roots_acc: 0.9487 - val_vowels_acc: 0.9818 - val_consonants_acc: 0.9801\n",
            "Epoch 3/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.3707 - roots_loss: 0.1982 - vowels_loss: 0.1023 - consonants_loss: 0.0702 - roots_acc: 0.9440 - vowels_acc: 0.9706 - consonants_acc: 0.9794Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 273us/sample - loss: 0.3784 - roots_loss: 0.1935 - vowels_loss: 0.0615 - consonants_loss: 0.0620 - roots_acc: 0.9487 - vowels_acc: 0.9861 - consonants_acc: 0.9828\n",
            "180/180 [==============================] - 45s 251ms/step - loss: 0.3709 - roots_loss: 0.1984 - vowels_loss: 0.1021 - consonants_loss: 0.0704 - roots_acc: 0.9440 - vowels_acc: 0.9706 - consonants_acc: 0.9794 - val_loss: 0.3171 - val_roots_loss: 0.1935 - val_vowels_loss: 0.0615 - val_consonants_loss: 0.0620 - val_roots_acc: 0.9487 - val_vowels_acc: 0.9861 - val_consonants_acc: 0.9828\n",
            "Epoch 4/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.3461 - roots_loss: 0.1805 - vowels_loss: 0.0970 - consonants_loss: 0.0685 - roots_acc: 0.9475 - vowels_acc: 0.9716 - consonants_acc: 0.9799Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 271us/sample - loss: 0.3532 - roots_loss: 0.1829 - vowels_loss: 0.0524 - consonants_loss: 0.0583 - roots_acc: 0.9520 - vowels_acc: 0.9876 - consonants_acc: 0.9838\n",
            "180/180 [==============================] - 45s 251ms/step - loss: 0.3460 - roots_loss: 0.1805 - vowels_loss: 0.0968 - consonants_loss: 0.0686 - roots_acc: 0.9475 - vowels_acc: 0.9716 - consonants_acc: 0.9800 - val_loss: 0.2936 - val_roots_loss: 0.1829 - val_vowels_loss: 0.0524 - val_consonants_loss: 0.0583 - val_roots_acc: 0.9520 - val_vowels_acc: 0.9876 - val_consonants_acc: 0.9838\n",
            "Epoch 5/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.3359 - roots_loss: 0.1732 - vowels_loss: 0.0983 - consonants_loss: 0.0645 - roots_acc: 0.9500 - vowels_acc: 0.9712 - consonants_acc: 0.9811Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 274us/sample - loss: 0.4141 - roots_loss: 0.1985 - vowels_loss: 0.0578 - consonants_loss: 0.0642 - roots_acc: 0.9507 - vowels_acc: 0.9858 - consonants_acc: 0.9826\n",
            "180/180 [==============================] - 45s 251ms/step - loss: 0.3365 - roots_loss: 0.1735 - vowels_loss: 0.0982 - consonants_loss: 0.0648 - roots_acc: 0.9498 - vowels_acc: 0.9712 - consonants_acc: 0.9810 - val_loss: 0.3205 - val_roots_loss: 0.1985 - val_vowels_loss: 0.0578 - val_consonants_loss: 0.0642 - val_roots_acc: 0.9507 - val_vowels_acc: 0.9858 - val_consonants_acc: 0.9826\n",
            "Epoch 6/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.3187 - roots_loss: 0.1653 - vowels_loss: 0.0889 - consonants_loss: 0.0646 - roots_acc: 0.9512 - vowels_acc: 0.9735 - consonants_acc: 0.9804Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 272us/sample - loss: 0.4002 - roots_loss: 0.1866 - vowels_loss: 0.0738 - consonants_loss: 0.0657 - roots_acc: 0.9542 - vowels_acc: 0.9803 - consonants_acc: 0.9816\n",
            "180/180 [==============================] - 45s 250ms/step - loss: 0.3191 - roots_loss: 0.1654 - vowels_loss: 0.0890 - consonants_loss: 0.0646 - roots_acc: 0.9511 - vowels_acc: 0.9735 - consonants_acc: 0.9804 - val_loss: 0.3262 - val_roots_loss: 0.1866 - val_vowels_loss: 0.0738 - val_consonants_loss: 0.0657 - val_roots_acc: 0.9542 - val_vowels_acc: 0.9803 - val_consonants_acc: 0.9816\n",
            "Epoch 7/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.3064 - roots_loss: 0.1538 - vowels_loss: 0.0906 - consonants_loss: 0.0619 - roots_acc: 0.9536 - vowels_acc: 0.9734 - consonants_acc: 0.9825Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 271us/sample - loss: 0.4769 - roots_loss: 0.2036 - vowels_loss: 0.1087 - consonants_loss: 0.0781 - roots_acc: 0.9460 - vowels_acc: 0.9666 - consonants_acc: 0.9766\n",
            "180/180 [==============================] - 45s 251ms/step - loss: 0.3061 - roots_loss: 0.1538 - vowels_loss: 0.0906 - consonants_loss: 0.0617 - roots_acc: 0.9536 - vowels_acc: 0.9734 - consonants_acc: 0.9826 - val_loss: 0.3904 - val_roots_loss: 0.2036 - val_vowels_loss: 0.1087 - val_consonants_loss: 0.0781 - val_roots_acc: 0.9460 - val_vowels_acc: 0.9666 - val_consonants_acc: 0.9766\n",
            "Epoch 8/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2957 - roots_loss: 0.1499 - vowels_loss: 0.0887 - consonants_loss: 0.0571 - roots_acc: 0.9554 - vowels_acc: 0.9740 - consonants_acc: 0.9827Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 272us/sample - loss: 0.3505 - roots_loss: 0.1865 - vowels_loss: 0.0576 - consonants_loss: 0.0579 - roots_acc: 0.9532 - vowels_acc: 0.9848 - consonants_acc: 0.9841\n",
            "180/180 [==============================] - 45s 251ms/step - loss: 0.2960 - roots_loss: 0.1499 - vowels_loss: 0.0889 - consonants_loss: 0.0571 - roots_acc: 0.9555 - vowels_acc: 0.9739 - consonants_acc: 0.9827 - val_loss: 0.3021 - val_roots_loss: 0.1865 - val_vowels_loss: 0.0576 - val_consonants_loss: 0.0579 - val_roots_acc: 0.9532 - val_vowels_acc: 0.9848 - val_consonants_acc: 0.9841\n",
            "Epoch 9/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2827 - roots_loss: 0.1418 - vowels_loss: 0.0853 - consonants_loss: 0.0556 - roots_acc: 0.9584 - vowels_acc: 0.9752 - consonants_acc: 0.9829Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 274us/sample - loss: 0.3844 - roots_loss: 0.1872 - vowels_loss: 0.0552 - consonants_loss: 0.0645 - roots_acc: 0.9522 - vowels_acc: 0.9868 - consonants_acc: 0.9823\n",
            "180/180 [==============================] - 45s 251ms/step - loss: 0.2834 - roots_loss: 0.1421 - vowels_loss: 0.0854 - consonants_loss: 0.0558 - roots_acc: 0.9582 - vowels_acc: 0.9752 - consonants_acc: 0.9829 - val_loss: 0.3069 - val_roots_loss: 0.1872 - val_vowels_loss: 0.0552 - val_consonants_loss: 0.0645 - val_roots_acc: 0.9522 - val_vowels_acc: 0.9868 - val_consonants_acc: 0.9823\n",
            "Epoch 10/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2673 - roots_loss: 0.1312 - vowels_loss: 0.0827 - consonants_loss: 0.0534 - roots_acc: 0.9597 - vowels_acc: 0.9761 - consonants_acc: 0.9831Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 272us/sample - loss: 0.4468 - roots_loss: 0.1950 - vowels_loss: 0.0869 - consonants_loss: 0.0668 - roots_acc: 0.9520 - vowels_acc: 0.9768 - consonants_acc: 0.9821\n",
            "180/180 [==============================] - 45s 251ms/step - loss: 0.2672 - roots_loss: 0.1311 - vowels_loss: 0.0826 - consonants_loss: 0.0535 - roots_acc: 0.9597 - vowels_acc: 0.9762 - consonants_acc: 0.9831 - val_loss: 0.3486 - val_roots_loss: 0.1950 - val_vowels_loss: 0.0869 - val_consonants_loss: 0.0668 - val_roots_acc: 0.9520 - val_vowels_acc: 0.9768 - val_consonants_acc: 0.9821\n",
            "Epoch 11/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2613 - roots_loss: 0.1290 - vowels_loss: 0.0791 - consonants_loss: 0.0532 - roots_acc: 0.9608 - vowels_acc: 0.9760 - consonants_acc: 0.9844Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 272us/sample - loss: 0.3468 - roots_loss: 0.1881 - vowels_loss: 0.0533 - consonants_loss: 0.0609 - roots_acc: 0.9515 - vowels_acc: 0.9871 - consonants_acc: 0.9833\n",
            "180/180 [==============================] - 45s 250ms/step - loss: 0.2610 - roots_loss: 0.1287 - vowels_loss: 0.0790 - consonants_loss: 0.0533 - roots_acc: 0.9609 - vowels_acc: 0.9760 - consonants_acc: 0.9843 - val_loss: 0.3022 - val_roots_loss: 0.1881 - val_vowels_loss: 0.0533 - val_consonants_loss: 0.0609 - val_roots_acc: 0.9515 - val_vowels_acc: 0.9871 - val_consonants_acc: 0.9833\n",
            "Epoch 12/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2504 - roots_loss: 0.1204 - vowels_loss: 0.0807 - consonants_loss: 0.0494 - roots_acc: 0.9623 - vowels_acc: 0.9765 - consonants_acc: 0.9852Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 273us/sample - loss: 0.3866 - roots_loss: 0.1884 - vowels_loss: 0.0599 - consonants_loss: 0.0591 - roots_acc: 0.9520 - vowels_acc: 0.9848 - consonants_acc: 0.9843\n",
            "180/180 [==============================] - 45s 251ms/step - loss: 0.2507 - roots_loss: 0.1205 - vowels_loss: 0.0807 - consonants_loss: 0.0494 - roots_acc: 0.9622 - vowels_acc: 0.9764 - consonants_acc: 0.9852 - val_loss: 0.3074 - val_roots_loss: 0.1884 - val_vowels_loss: 0.0599 - val_consonants_loss: 0.0591 - val_roots_acc: 0.9520 - val_vowels_acc: 0.9848 - val_consonants_acc: 0.9843\n",
            "Epoch 13/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2508 - roots_loss: 0.1196 - vowels_loss: 0.0793 - consonants_loss: 0.0519 - roots_acc: 0.9636 - vowels_acc: 0.9762 - consonants_acc: 0.9845Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 273us/sample - loss: 0.4152 - roots_loss: 0.1952 - vowels_loss: 0.0662 - consonants_loss: 0.0602 - roots_acc: 0.9559 - vowels_acc: 0.9831 - consonants_acc: 0.9853\n",
            "180/180 [==============================] - 45s 251ms/step - loss: 0.2507 - roots_loss: 0.1197 - vowels_loss: 0.0790 - consonants_loss: 0.0519 - roots_acc: 0.9636 - vowels_acc: 0.9763 - consonants_acc: 0.9845 - val_loss: 0.3216 - val_roots_loss: 0.1952 - val_vowels_loss: 0.0662 - val_consonants_loss: 0.0602 - val_roots_acc: 0.9559 - val_vowels_acc: 0.9831 - val_consonants_acc: 0.9853\n",
            "Epoch 14/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2428 - roots_loss: 0.1152 - vowels_loss: 0.0769 - consonants_loss: 0.0508 - roots_acc: 0.9646 - vowels_acc: 0.9769 - consonants_acc: 0.9847Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 271us/sample - loss: 0.3926 - roots_loss: 0.2129 - vowels_loss: 0.0626 - consonants_loss: 0.0633 - roots_acc: 0.9502 - vowels_acc: 0.9866 - consonants_acc: 0.9843\n",
            "180/180 [==============================] - 45s 250ms/step - loss: 0.2425 - roots_loss: 0.1148 - vowels_loss: 0.0772 - consonants_loss: 0.0506 - roots_acc: 0.9648 - vowels_acc: 0.9769 - consonants_acc: 0.9847 - val_loss: 0.3388 - val_roots_loss: 0.2129 - val_vowels_loss: 0.0626 - val_consonants_loss: 0.0633 - val_roots_acc: 0.9502 - val_vowels_acc: 0.9866 - val_consonants_acc: 0.9843\n",
            "Epoch 15/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2294 - roots_loss: 0.1068 - vowels_loss: 0.0744 - consonants_loss: 0.0483 - roots_acc: 0.9668 - vowels_acc: 0.9775 - consonants_acc: 0.9854Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 272us/sample - loss: 0.3873 - roots_loss: 0.2007 - vowels_loss: 0.0624 - consonants_loss: 0.0645 - roots_acc: 0.9532 - vowels_acc: 0.9863 - consonants_acc: 0.9851\n",
            "180/180 [==============================] - 45s 251ms/step - loss: 0.2296 - roots_loss: 0.1067 - vowels_loss: 0.0745 - consonants_loss: 0.0484 - roots_acc: 0.9668 - vowels_acc: 0.9774 - consonants_acc: 0.9854 - val_loss: 0.3276 - val_roots_loss: 0.2007 - val_vowels_loss: 0.0624 - val_consonants_loss: 0.0645 - val_roots_acc: 0.9532 - val_vowels_acc: 0.9863 - val_consonants_acc: 0.9851\n",
            "Epoch 16/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2307 - roots_loss: 0.1076 - vowels_loss: 0.0743 - consonants_loss: 0.0488 - roots_acc: 0.9666 - vowels_acc: 0.9784 - consonants_acc: 0.9859Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 270us/sample - loss: 0.3711 - roots_loss: 0.1964 - vowels_loss: 0.0555 - consonants_loss: 0.0655 - roots_acc: 0.9517 - vowels_acc: 0.9871 - consonants_acc: 0.9823\n",
            "180/180 [==============================] - 45s 250ms/step - loss: 0.2305 - roots_loss: 0.1076 - vowels_loss: 0.0742 - consonants_loss: 0.0487 - roots_acc: 0.9666 - vowels_acc: 0.9785 - consonants_acc: 0.9859 - val_loss: 0.3173 - val_roots_loss: 0.1964 - val_vowels_loss: 0.0555 - val_consonants_loss: 0.0655 - val_roots_acc: 0.9517 - val_vowels_acc: 0.9871 - val_consonants_acc: 0.9823\n",
            "Epoch 17/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2277 - roots_loss: 0.1070 - vowels_loss: 0.0749 - consonants_loss: 0.0457 - roots_acc: 0.9662 - vowels_acc: 0.9772 - consonants_acc: 0.9861Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 272us/sample - loss: 0.3885 - roots_loss: 0.2015 - vowels_loss: 0.0616 - consonants_loss: 0.0651 - roots_acc: 0.9510 - vowels_acc: 0.9871 - consonants_acc: 0.9841\n",
            "180/180 [==============================] - 45s 250ms/step - loss: 0.2274 - roots_loss: 0.1069 - vowels_loss: 0.0749 - consonants_loss: 0.0456 - roots_acc: 0.9662 - vowels_acc: 0.9772 - consonants_acc: 0.9861 - val_loss: 0.3282 - val_roots_loss: 0.2015 - val_vowels_loss: 0.0616 - val_consonants_loss: 0.0651 - val_roots_acc: 0.9510 - val_vowels_acc: 0.9871 - val_consonants_acc: 0.9841\n",
            "Epoch 18/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2169 - roots_loss: 0.0998 - vowels_loss: 0.0733 - consonants_loss: 0.0439 - roots_acc: 0.9680 - vowels_acc: 0.9780 - consonants_acc: 0.9864Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 270us/sample - loss: 0.3891 - roots_loss: 0.2026 - vowels_loss: 0.0659 - consonants_loss: 0.0635 - roots_acc: 0.9525 - vowels_acc: 0.9856 - consonants_acc: 0.9858\n",
            "180/180 [==============================] - 45s 250ms/step - loss: 0.2172 - roots_loss: 0.0996 - vowels_loss: 0.0735 - consonants_loss: 0.0441 - roots_acc: 0.9680 - vowels_acc: 0.9780 - consonants_acc: 0.9863 - val_loss: 0.3320 - val_roots_loss: 0.2026 - val_vowels_loss: 0.0659 - val_consonants_loss: 0.0635 - val_roots_acc: 0.9525 - val_vowels_acc: 0.9856 - val_consonants_acc: 0.9858\n",
            "Epoch 19/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2178 - roots_loss: 0.0993 - vowels_loss: 0.0719 - consonants_loss: 0.0467 - roots_acc: 0.9692 - vowels_acc: 0.9787 - consonants_acc: 0.9863Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 272us/sample - loss: 0.4410 - roots_loss: 0.2047 - vowels_loss: 0.0726 - consonants_loss: 0.0658 - roots_acc: 0.9515 - vowels_acc: 0.9858 - consonants_acc: 0.9838\n",
            "180/180 [==============================] - 45s 250ms/step - loss: 0.2184 - roots_loss: 0.0998 - vowels_loss: 0.0719 - consonants_loss: 0.0467 - roots_acc: 0.9691 - vowels_acc: 0.9786 - consonants_acc: 0.9863 - val_loss: 0.3430 - val_roots_loss: 0.2047 - val_vowels_loss: 0.0726 - val_consonants_loss: 0.0658 - val_roots_acc: 0.9515 - val_vowels_acc: 0.9858 - val_consonants_acc: 0.9838\n",
            "Epoch 20/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2096 - roots_loss: 0.0967 - vowels_loss: 0.0699 - consonants_loss: 0.0430 - roots_acc: 0.9691 - vowels_acc: 0.9788 - consonants_acc: 0.9871Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 276us/sample - loss: 0.3994 - roots_loss: 0.2114 - vowels_loss: 0.0659 - consonants_loss: 0.0695 - roots_acc: 0.9520 - vowels_acc: 0.9853 - consonants_acc: 0.9821\n",
            "180/180 [==============================] - 45s 251ms/step - loss: 0.2101 - roots_loss: 0.0967 - vowels_loss: 0.0702 - consonants_loss: 0.0433 - roots_acc: 0.9692 - vowels_acc: 0.9788 - consonants_acc: 0.9870 - val_loss: 0.3468 - val_roots_loss: 0.2114 - val_vowels_loss: 0.0659 - val_consonants_loss: 0.0695 - val_roots_acc: 0.9520 - val_vowels_acc: 0.9853 - val_consonants_acc: 0.9821\n",
            "Epoch 21/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.1997 - roots_loss: 0.0904 - vowels_loss: 0.0692 - consonants_loss: 0.0401 - roots_acc: 0.9716 - vowels_acc: 0.9787 - consonants_acc: 0.9877Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 283us/sample - loss: 0.4269 - roots_loss: 0.2103 - vowels_loss: 0.0638 - consonants_loss: 0.0705 - roots_acc: 0.9495 - vowels_acc: 0.9861 - consonants_acc: 0.9821\n",
            "180/180 [==============================] - 45s 252ms/step - loss: 0.2000 - roots_loss: 0.0907 - vowels_loss: 0.0691 - consonants_loss: 0.0402 - roots_acc: 0.9715 - vowels_acc: 0.9786 - consonants_acc: 0.9877 - val_loss: 0.3446 - val_roots_loss: 0.2103 - val_vowels_loss: 0.0638 - val_consonants_loss: 0.0705 - val_roots_acc: 0.9495 - val_vowels_acc: 0.9861 - val_consonants_acc: 0.9821\n",
            "Epoch 22/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2069 - roots_loss: 0.0929 - vowels_loss: 0.0694 - consonants_loss: 0.0445 - roots_acc: 0.9711 - vowels_acc: 0.9791 - consonants_acc: 0.9870Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 277us/sample - loss: 0.4330 - roots_loss: 0.2081 - vowels_loss: 0.0626 - consonants_loss: 0.0655 - roots_acc: 0.9515 - vowels_acc: 0.9873 - consonants_acc: 0.9833\n",
            "180/180 [==============================] - 45s 252ms/step - loss: 0.2065 - roots_loss: 0.0928 - vowels_loss: 0.0693 - consonants_loss: 0.0444 - roots_acc: 0.9711 - vowels_acc: 0.9791 - consonants_acc: 0.9870 - val_loss: 0.3361 - val_roots_loss: 0.2081 - val_vowels_loss: 0.0626 - val_consonants_loss: 0.0655 - val_roots_acc: 0.9515 - val_vowels_acc: 0.9873 - val_consonants_acc: 0.9833\n",
            "Epoch 23/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.1966 - roots_loss: 0.0878 - vowels_loss: 0.0678 - consonants_loss: 0.0409 - roots_acc: 0.9723 - vowels_acc: 0.9796 - consonants_acc: 0.9874Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 276us/sample - loss: 0.4691 - roots_loss: 0.2354 - vowels_loss: 0.0766 - consonants_loss: 0.0710 - roots_acc: 0.9487 - vowels_acc: 0.9831 - consonants_acc: 0.9828\n",
            "180/180 [==============================] - 45s 252ms/step - loss: 0.1966 - roots_loss: 0.0880 - vowels_loss: 0.0676 - consonants_loss: 0.0410 - roots_acc: 0.9722 - vowels_acc: 0.9796 - consonants_acc: 0.9873 - val_loss: 0.3830 - val_roots_loss: 0.2354 - val_vowels_loss: 0.0766 - val_consonants_loss: 0.0710 - val_roots_acc: 0.9487 - val_vowels_acc: 0.9831 - val_consonants_acc: 0.9828\n",
            "Epoch 24/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.1917 - roots_loss: 0.0844 - vowels_loss: 0.0679 - consonants_loss: 0.0394 - roots_acc: 0.9736 - vowels_acc: 0.9797 - consonants_acc: 0.9878Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 273us/sample - loss: 0.4556 - roots_loss: 0.2252 - vowels_loss: 0.0720 - consonants_loss: 0.0809 - roots_acc: 0.9477 - vowels_acc: 0.9813 - consonants_acc: 0.9796\n",
            "180/180 [==============================] - 45s 252ms/step - loss: 0.1915 - roots_loss: 0.0843 - vowels_loss: 0.0679 - consonants_loss: 0.0392 - roots_acc: 0.9736 - vowels_acc: 0.9797 - consonants_acc: 0.9878 - val_loss: 0.3780 - val_roots_loss: 0.2252 - val_vowels_loss: 0.0720 - val_consonants_loss: 0.0809 - val_roots_acc: 0.9477 - val_vowels_acc: 0.9813 - val_consonants_acc: 0.9796\n",
            "Epoch 25/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.1865 - roots_loss: 0.0821 - vowels_loss: 0.0649 - consonants_loss: 0.0396 - roots_acc: 0.9737 - vowels_acc: 0.9802 - consonants_acc: 0.9880Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 275us/sample - loss: 0.4228 - roots_loss: 0.2172 - vowels_loss: 0.0617 - consonants_loss: 0.0659 - roots_acc: 0.9507 - vowels_acc: 0.9861 - consonants_acc: 0.9851\n",
            "180/180 [==============================] - 45s 251ms/step - loss: 0.1867 - roots_loss: 0.0820 - vowels_loss: 0.0650 - consonants_loss: 0.0397 - roots_acc: 0.9737 - vowels_acc: 0.9802 - consonants_acc: 0.9880 - val_loss: 0.3449 - val_roots_loss: 0.2172 - val_vowels_loss: 0.0617 - val_consonants_loss: 0.0659 - val_roots_acc: 0.9507 - val_vowels_acc: 0.9861 - val_consonants_acc: 0.9851\n",
            "Epoch 26/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.1872 - roots_loss: 0.0810 - vowels_loss: 0.0658 - consonants_loss: 0.0404 - roots_acc: 0.9740 - vowels_acc: 0.9803 - consonants_acc: 0.9877Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 274us/sample - loss: 0.4071 - roots_loss: 0.2119 - vowels_loss: 0.0632 - consonants_loss: 0.0693 - roots_acc: 0.9529 - vowels_acc: 0.9871 - consonants_acc: 0.9833\n",
            "180/180 [==============================] - 45s 252ms/step - loss: 0.1871 - roots_loss: 0.0809 - vowels_loss: 0.0659 - consonants_loss: 0.0402 - roots_acc: 0.9740 - vowels_acc: 0.9803 - consonants_acc: 0.9878 - val_loss: 0.3445 - val_roots_loss: 0.2119 - val_vowels_loss: 0.0632 - val_consonants_loss: 0.0693 - val_roots_acc: 0.9529 - val_vowels_acc: 0.9871 - val_consonants_acc: 0.9833\n",
            "Epoch 27/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.1824 - roots_loss: 0.0803 - vowels_loss: 0.0652 - consonants_loss: 0.0369 - roots_acc: 0.9749 - vowels_acc: 0.9803 - consonants_acc: 0.9890Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 276us/sample - loss: 0.4407 - roots_loss: 0.2261 - vowels_loss: 0.0685 - consonants_loss: 0.0731 - roots_acc: 0.9492 - vowels_acc: 0.9866 - consonants_acc: 0.9828\n",
            "180/180 [==============================] - 45s 252ms/step - loss: 0.1830 - roots_loss: 0.0804 - vowels_loss: 0.0653 - consonants_loss: 0.0372 - roots_acc: 0.9748 - vowels_acc: 0.9803 - consonants_acc: 0.9889 - val_loss: 0.3677 - val_roots_loss: 0.2261 - val_vowels_loss: 0.0685 - val_consonants_loss: 0.0731 - val_roots_acc: 0.9492 - val_vowels_acc: 0.9866 - val_consonants_acc: 0.9828\n",
            "Epoch 28/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.1842 - roots_loss: 0.0785 - vowels_loss: 0.0676 - consonants_loss: 0.0380 - roots_acc: 0.9747 - vowels_acc: 0.9796 - consonants_acc: 0.9882Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 274us/sample - loss: 0.4254 - roots_loss: 0.2244 - vowels_loss: 0.0654 - consonants_loss: 0.0740 - roots_acc: 0.9525 - vowels_acc: 0.9851 - consonants_acc: 0.9833\n",
            "\n",
            "Epoch 00028: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
            "180/180 [==============================] - 45s 252ms/step - loss: 0.1840 - roots_loss: 0.0785 - vowels_loss: 0.0676 - consonants_loss: 0.0379 - roots_acc: 0.9747 - vowels_acc: 0.9796 - consonants_acc: 0.9882 - val_loss: 0.3637 - val_roots_loss: 0.2244 - val_vowels_loss: 0.0654 - val_consonants_loss: 0.0740 - val_roots_acc: 0.9525 - val_vowels_acc: 0.9851 - val_consonants_acc: 0.9833\n",
            "Epoch 29/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.1718 - roots_loss: 0.0711 - vowels_loss: 0.0635 - consonants_loss: 0.0371 - roots_acc: 0.9779 - vowels_acc: 0.9813 - consonants_acc: 0.9886Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 273us/sample - loss: 0.4332 - roots_loss: 0.2166 - vowels_loss: 0.0649 - consonants_loss: 0.0747 - roots_acc: 0.9534 - vowels_acc: 0.9853 - consonants_acc: 0.9831\n",
            "180/180 [==============================] - 45s 250ms/step - loss: 0.1717 - roots_loss: 0.0711 - vowels_loss: 0.0634 - consonants_loss: 0.0372 - roots_acc: 0.9779 - vowels_acc: 0.9814 - consonants_acc: 0.9886 - val_loss: 0.3562 - val_roots_loss: 0.2166 - val_vowels_loss: 0.0649 - val_consonants_loss: 0.0747 - val_roots_acc: 0.9534 - val_vowels_acc: 0.9853 - val_consonants_acc: 0.9831\n",
            "Epoch 30/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.1568 - roots_loss: 0.0666 - vowels_loss: 0.0589 - consonants_loss: 0.0314 - roots_acc: 0.9787 - vowels_acc: 0.9824 - consonants_acc: 0.9905Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 273us/sample - loss: 0.4252 - roots_loss: 0.2161 - vowels_loss: 0.0649 - consonants_loss: 0.0750 - roots_acc: 0.9539 - vowels_acc: 0.9861 - consonants_acc: 0.9848\n",
            "180/180 [==============================] - 45s 250ms/step - loss: 0.1567 - roots_loss: 0.0665 - vowels_loss: 0.0587 - consonants_loss: 0.0314 - roots_acc: 0.9787 - vowels_acc: 0.9825 - consonants_acc: 0.9905 - val_loss: 0.3561 - val_roots_loss: 0.2161 - val_vowels_loss: 0.0649 - val_consonants_loss: 0.0750 - val_roots_acc: 0.9539 - val_vowels_acc: 0.9861 - val_consonants_acc: 0.9848\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "12f5900e2df345e6bafa3f88136af10b",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=50210), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training images: (50210, 64, 64, 1)\n",
            "Training labels root: (50210, 168)\n",
            "Training labels vowel: (50210, 11)\n",
            "Training labels consonants: (50210, 7)\n",
            "Epoch 1/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.4482 - roots_loss: 0.2571 - vowels_loss: 0.1112 - consonants_loss: 0.0800 - roots_acc: 0.9361 - vowels_acc: 0.9692 - consonants_acc: 0.9779Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 272us/sample - loss: 0.2656 - roots_loss: 0.1805 - vowels_loss: 0.0710 - consonants_loss: 0.0472 - roots_acc: 0.9517 - vowels_acc: 0.9833 - consonants_acc: 0.9885\n",
            "180/180 [==============================] - 46s 253ms/step - loss: 0.4485 - roots_loss: 0.2573 - vowels_loss: 0.1111 - consonants_loss: 0.0801 - roots_acc: 0.9360 - vowels_acc: 0.9692 - consonants_acc: 0.9779 - val_loss: 0.2987 - val_roots_loss: 0.1805 - val_vowels_loss: 0.0710 - val_consonants_loss: 0.0472 - val_roots_acc: 0.9517 - val_vowels_acc: 0.9833 - val_consonants_acc: 0.9885\n",
            "Epoch 2/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.3846 - roots_loss: 0.2107 - vowels_loss: 0.1029 - consonants_loss: 0.0709 - roots_acc: 0.9420 - vowels_acc: 0.9712 - consonants_acc: 0.9801Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 271us/sample - loss: 0.2442 - roots_loss: 0.1689 - vowels_loss: 0.0673 - consonants_loss: 0.0439 - roots_acc: 0.9532 - vowels_acc: 0.9848 - consonants_acc: 0.9883\n",
            "180/180 [==============================] - 45s 250ms/step - loss: 0.3843 - roots_loss: 0.2108 - vowels_loss: 0.1027 - consonants_loss: 0.0707 - roots_acc: 0.9421 - vowels_acc: 0.9712 - consonants_acc: 0.9801 - val_loss: 0.2802 - val_roots_loss: 0.1689 - val_vowels_loss: 0.0673 - val_consonants_loss: 0.0439 - val_roots_acc: 0.9532 - val_vowels_acc: 0.9848 - val_consonants_acc: 0.9883\n",
            "Epoch 3/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.3601 - roots_loss: 0.1943 - vowels_loss: 0.0968 - consonants_loss: 0.0690 - roots_acc: 0.9449 - vowels_acc: 0.9727 - consonants_acc: 0.9801Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 271us/sample - loss: 0.2234 - roots_loss: 0.1655 - vowels_loss: 0.0632 - consonants_loss: 0.0404 - roots_acc: 0.9522 - vowels_acc: 0.9866 - consonants_acc: 0.9895\n",
            "180/180 [==============================] - 45s 250ms/step - loss: 0.3595 - roots_loss: 0.1938 - vowels_loss: 0.0966 - consonants_loss: 0.0690 - roots_acc: 0.9450 - vowels_acc: 0.9727 - consonants_acc: 0.9801 - val_loss: 0.2690 - val_roots_loss: 0.1655 - val_vowels_loss: 0.0632 - val_consonants_loss: 0.0404 - val_roots_acc: 0.9522 - val_vowels_acc: 0.9866 - val_consonants_acc: 0.9895\n",
            "Epoch 4/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.3422 - roots_loss: 0.1864 - vowels_loss: 0.0918 - consonants_loss: 0.0640 - roots_acc: 0.9482 - vowels_acc: 0.9739 - consonants_acc: 0.9817Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 271us/sample - loss: 0.2357 - roots_loss: 0.1651 - vowels_loss: 0.0635 - consonants_loss: 0.0417 - roots_acc: 0.9547 - vowels_acc: 0.9858 - consonants_acc: 0.9895\n",
            "180/180 [==============================] - 45s 250ms/step - loss: 0.3427 - roots_loss: 0.1865 - vowels_loss: 0.0921 - consonants_loss: 0.0640 - roots_acc: 0.9481 - vowels_acc: 0.9739 - consonants_acc: 0.9816 - val_loss: 0.2703 - val_roots_loss: 0.1651 - val_vowels_loss: 0.0635 - val_consonants_loss: 0.0417 - val_roots_acc: 0.9547 - val_vowels_acc: 0.9858 - val_consonants_acc: 0.9895\n",
            "Epoch 5/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.3262 - roots_loss: 0.1723 - vowels_loss: 0.0923 - consonants_loss: 0.0616 - roots_acc: 0.9507 - vowels_acc: 0.9738 - consonants_acc: 0.9819Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 273us/sample - loss: 0.2697 - roots_loss: 0.1715 - vowels_loss: 0.0885 - consonants_loss: 0.0494 - roots_acc: 0.9495 - vowels_acc: 0.9768 - consonants_acc: 0.9868\n",
            "180/180 [==============================] - 45s 250ms/step - loss: 0.3261 - roots_loss: 0.1722 - vowels_loss: 0.0922 - consonants_loss: 0.0616 - roots_acc: 0.9507 - vowels_acc: 0.9738 - consonants_acc: 0.9818 - val_loss: 0.3094 - val_roots_loss: 0.1715 - val_vowels_loss: 0.0885 - val_consonants_loss: 0.0494 - val_roots_acc: 0.9495 - val_vowels_acc: 0.9768 - val_consonants_acc: 0.9868\n",
            "Epoch 6/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.3164 - roots_loss: 0.1636 - vowels_loss: 0.0915 - consonants_loss: 0.0612 - roots_acc: 0.9529 - vowels_acc: 0.9720 - consonants_acc: 0.9821Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 271us/sample - loss: 0.2223 - roots_loss: 0.1616 - vowels_loss: 0.0659 - consonants_loss: 0.0400 - roots_acc: 0.9542 - vowels_acc: 0.9853 - consonants_acc: 0.9898\n",
            "180/180 [==============================] - 45s 251ms/step - loss: 0.3167 - roots_loss: 0.1642 - vowels_loss: 0.0915 - consonants_loss: 0.0610 - roots_acc: 0.9528 - vowels_acc: 0.9720 - consonants_acc: 0.9822 - val_loss: 0.2675 - val_roots_loss: 0.1616 - val_vowels_loss: 0.0659 - val_consonants_loss: 0.0400 - val_roots_acc: 0.9542 - val_vowels_acc: 0.9853 - val_consonants_acc: 0.9898\n",
            "Epoch 7/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.3041 - roots_loss: 0.1538 - vowels_loss: 0.0893 - consonants_loss: 0.0610 - roots_acc: 0.9548 - vowels_acc: 0.9743 - consonants_acc: 0.9829Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 273us/sample - loss: 0.2760 - roots_loss: 0.1667 - vowels_loss: 0.1085 - consonants_loss: 0.0471 - roots_acc: 0.9532 - vowels_acc: 0.9654 - consonants_acc: 0.9885\n",
            "180/180 [==============================] - 45s 250ms/step - loss: 0.3044 - roots_loss: 0.1540 - vowels_loss: 0.0895 - consonants_loss: 0.0610 - roots_acc: 0.9547 - vowels_acc: 0.9742 - consonants_acc: 0.9829 - val_loss: 0.3223 - val_roots_loss: 0.1667 - val_vowels_loss: 0.1085 - val_consonants_loss: 0.0471 - val_roots_acc: 0.9532 - val_vowels_acc: 0.9654 - val_consonants_acc: 0.9885\n",
            "Epoch 8/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2889 - roots_loss: 0.1499 - vowels_loss: 0.0833 - consonants_loss: 0.0556 - roots_acc: 0.9555 - vowels_acc: 0.9755 - consonants_acc: 0.9833Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 276us/sample - loss: 0.2411 - roots_loss: 0.1629 - vowels_loss: 0.0698 - consonants_loss: 0.0428 - roots_acc: 0.9552 - vowels_acc: 0.9843 - consonants_acc: 0.9878\n",
            "180/180 [==============================] - 45s 252ms/step - loss: 0.2885 - roots_loss: 0.1498 - vowels_loss: 0.0831 - consonants_loss: 0.0555 - roots_acc: 0.9555 - vowels_acc: 0.9756 - consonants_acc: 0.9834 - val_loss: 0.2755 - val_roots_loss: 0.1629 - val_vowels_loss: 0.0698 - val_consonants_loss: 0.0428 - val_roots_acc: 0.9552 - val_vowels_acc: 0.9843 - val_consonants_acc: 0.9878\n",
            "Epoch 9/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2851 - roots_loss: 0.1440 - vowels_loss: 0.0866 - consonants_loss: 0.0545 - roots_acc: 0.9576 - vowels_acc: 0.9749 - consonants_acc: 0.9844Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 272us/sample - loss: 0.2337 - roots_loss: 0.1655 - vowels_loss: 0.0663 - consonants_loss: 0.0407 - roots_acc: 0.9532 - vowels_acc: 0.9836 - consonants_acc: 0.9883\n",
            "180/180 [==============================] - 45s 252ms/step - loss: 0.2853 - roots_loss: 0.1442 - vowels_loss: 0.0866 - consonants_loss: 0.0546 - roots_acc: 0.9577 - vowels_acc: 0.9749 - consonants_acc: 0.9844 - val_loss: 0.2724 - val_roots_loss: 0.1655 - val_vowels_loss: 0.0663 - val_consonants_loss: 0.0407 - val_roots_acc: 0.9532 - val_vowels_acc: 0.9836 - val_consonants_acc: 0.9883\n",
            "Epoch 10/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2805 - roots_loss: 0.1430 - vowels_loss: 0.0813 - consonants_loss: 0.0563 - roots_acc: 0.9584 - vowels_acc: 0.9763 - consonants_acc: 0.9833Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 272us/sample - loss: 0.2309 - roots_loss: 0.1648 - vowels_loss: 0.0662 - consonants_loss: 0.0396 - roots_acc: 0.9552 - vowels_acc: 0.9841 - consonants_acc: 0.9900\n",
            "180/180 [==============================] - 45s 251ms/step - loss: 0.2805 - roots_loss: 0.1428 - vowels_loss: 0.0814 - consonants_loss: 0.0562 - roots_acc: 0.9584 - vowels_acc: 0.9763 - consonants_acc: 0.9833 - val_loss: 0.2706 - val_roots_loss: 0.1648 - val_vowels_loss: 0.0662 - val_consonants_loss: 0.0396 - val_roots_acc: 0.9552 - val_vowels_acc: 0.9841 - val_consonants_acc: 0.9900\n",
            "Epoch 11/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2712 - roots_loss: 0.1378 - vowels_loss: 0.0807 - consonants_loss: 0.0527 - roots_acc: 0.9584 - vowels_acc: 0.9759 - consonants_acc: 0.9844Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 275us/sample - loss: 0.2331 - roots_loss: 0.1681 - vowels_loss: 0.0675 - consonants_loss: 0.0418 - roots_acc: 0.9552 - vowels_acc: 0.9846 - consonants_acc: 0.9885\n",
            "180/180 [==============================] - 45s 251ms/step - loss: 0.2707 - roots_loss: 0.1374 - vowels_loss: 0.0807 - consonants_loss: 0.0526 - roots_acc: 0.9584 - vowels_acc: 0.9759 - consonants_acc: 0.9844 - val_loss: 0.2774 - val_roots_loss: 0.1681 - val_vowels_loss: 0.0675 - val_consonants_loss: 0.0418 - val_roots_acc: 0.9552 - val_vowels_acc: 0.9846 - val_consonants_acc: 0.9885\n",
            "Epoch 12/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2655 - roots_loss: 0.1328 - vowels_loss: 0.0812 - consonants_loss: 0.0515 - roots_acc: 0.9600 - vowels_acc: 0.9761 - consonants_acc: 0.9852Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 273us/sample - loss: 0.2374 - roots_loss: 0.1754 - vowels_loss: 0.0704 - consonants_loss: 0.0419 - roots_acc: 0.9495 - vowels_acc: 0.9843 - consonants_acc: 0.9890\n",
            "180/180 [==============================] - 45s 251ms/step - loss: 0.2659 - roots_loss: 0.1329 - vowels_loss: 0.0813 - consonants_loss: 0.0516 - roots_acc: 0.9601 - vowels_acc: 0.9761 - consonants_acc: 0.9852 - val_loss: 0.2877 - val_roots_loss: 0.1754 - val_vowels_loss: 0.0704 - val_consonants_loss: 0.0419 - val_roots_acc: 0.9495 - val_vowels_acc: 0.9843 - val_consonants_acc: 0.9890\n",
            "Epoch 13/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2601 - roots_loss: 0.1260 - vowels_loss: 0.0804 - consonants_loss: 0.0537 - roots_acc: 0.9614 - vowels_acc: 0.9764 - consonants_acc: 0.9846Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 276us/sample - loss: 0.2380 - roots_loss: 0.1785 - vowels_loss: 0.0697 - consonants_loss: 0.0439 - roots_acc: 0.9520 - vowels_acc: 0.9846 - consonants_acc: 0.9876\n",
            "\n",
            "Epoch 00013: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
            "180/180 [==============================] - 45s 251ms/step - loss: 0.2599 - roots_loss: 0.1260 - vowels_loss: 0.0804 - consonants_loss: 0.0535 - roots_acc: 0.9614 - vowels_acc: 0.9764 - consonants_acc: 0.9847 - val_loss: 0.2921 - val_roots_loss: 0.1785 - val_vowels_loss: 0.0697 - val_consonants_loss: 0.0439 - val_roots_acc: 0.9520 - val_vowels_acc: 0.9846 - val_consonants_acc: 0.9876\n",
            "Epoch 14/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2451 - roots_loss: 0.1188 - vowels_loss: 0.0771 - consonants_loss: 0.0491 - roots_acc: 0.9644 - vowels_acc: 0.9767 - consonants_acc: 0.9856Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 276us/sample - loss: 0.2471 - roots_loss: 0.1717 - vowels_loss: 0.0711 - consonants_loss: 0.0418 - roots_acc: 0.9534 - vowels_acc: 0.9831 - consonants_acc: 0.9900\n",
            "180/180 [==============================] - 45s 250ms/step - loss: 0.2450 - roots_loss: 0.1187 - vowels_loss: 0.0771 - consonants_loss: 0.0491 - roots_acc: 0.9645 - vowels_acc: 0.9767 - consonants_acc: 0.9855 - val_loss: 0.2847 - val_roots_loss: 0.1717 - val_vowels_loss: 0.0711 - val_consonants_loss: 0.0418 - val_roots_acc: 0.9534 - val_vowels_acc: 0.9831 - val_consonants_acc: 0.9900\n",
            "Epoch 15/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2387 - roots_loss: 0.1173 - vowels_loss: 0.0730 - consonants_loss: 0.0484 - roots_acc: 0.9640 - vowels_acc: 0.9776 - consonants_acc: 0.9861Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 271us/sample - loss: 0.2346 - roots_loss: 0.1728 - vowels_loss: 0.0695 - consonants_loss: 0.0390 - roots_acc: 0.9542 - vowels_acc: 0.9838 - consonants_acc: 0.9908\n",
            "180/180 [==============================] - 45s 251ms/step - loss: 0.2386 - roots_loss: 0.1176 - vowels_loss: 0.0727 - consonants_loss: 0.0483 - roots_acc: 0.9640 - vowels_acc: 0.9778 - consonants_acc: 0.9861 - val_loss: 0.2813 - val_roots_loss: 0.1728 - val_vowels_loss: 0.0695 - val_consonants_loss: 0.0390 - val_roots_acc: 0.9542 - val_vowels_acc: 0.9838 - val_consonants_acc: 0.9908\n",
            "Epoch 16/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2436 - roots_loss: 0.1174 - vowels_loss: 0.0764 - consonants_loss: 0.0498 - roots_acc: 0.9644 - vowels_acc: 0.9764 - consonants_acc: 0.9848Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 271us/sample - loss: 0.2392 - roots_loss: 0.1722 - vowels_loss: 0.0698 - consonants_loss: 0.0398 - roots_acc: 0.9537 - vowels_acc: 0.9843 - consonants_acc: 0.9908\n",
            "180/180 [==============================] - 45s 250ms/step - loss: 0.2434 - roots_loss: 0.1174 - vowels_loss: 0.0763 - consonants_loss: 0.0497 - roots_acc: 0.9645 - vowels_acc: 0.9764 - consonants_acc: 0.9848 - val_loss: 0.2818 - val_roots_loss: 0.1722 - val_vowels_loss: 0.0698 - val_consonants_loss: 0.0398 - val_roots_acc: 0.9537 - val_vowels_acc: 0.9843 - val_consonants_acc: 0.9908\n",
            "Epoch 17/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2374 - roots_loss: 0.1133 - vowels_loss: 0.0763 - consonants_loss: 0.0479 - roots_acc: 0.9647 - vowels_acc: 0.9781 - consonants_acc: 0.9858Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 275us/sample - loss: 0.2354 - roots_loss: 0.1687 - vowels_loss: 0.0686 - consonants_loss: 0.0417 - roots_acc: 0.9542 - vowels_acc: 0.9843 - consonants_acc: 0.9903\n",
            "180/180 [==============================] - 45s 251ms/step - loss: 0.2377 - roots_loss: 0.1137 - vowels_loss: 0.0762 - consonants_loss: 0.0479 - roots_acc: 0.9647 - vowels_acc: 0.9781 - consonants_acc: 0.9858 - val_loss: 0.2789 - val_roots_loss: 0.1687 - val_vowels_loss: 0.0686 - val_consonants_loss: 0.0417 - val_roots_acc: 0.9542 - val_vowels_acc: 0.9843 - val_consonants_acc: 0.9903\n",
            "Epoch 18/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2321 - roots_loss: 0.1128 - vowels_loss: 0.0724 - consonants_loss: 0.0469 - roots_acc: 0.9656 - vowels_acc: 0.9786 - consonants_acc: 0.9866Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 272us/sample - loss: 0.2419 - roots_loss: 0.1721 - vowels_loss: 0.0689 - consonants_loss: 0.0419 - roots_acc: 0.9547 - vowels_acc: 0.9848 - consonants_acc: 0.9893\n",
            "180/180 [==============================] - 45s 250ms/step - loss: 0.2316 - roots_loss: 0.1124 - vowels_loss: 0.0722 - consonants_loss: 0.0470 - roots_acc: 0.9657 - vowels_acc: 0.9787 - consonants_acc: 0.9866 - val_loss: 0.2829 - val_roots_loss: 0.1721 - val_vowels_loss: 0.0689 - val_consonants_loss: 0.0419 - val_roots_acc: 0.9547 - val_vowels_acc: 0.9848 - val_consonants_acc: 0.9893\n",
            "Epoch 19/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2278 - roots_loss: 0.1102 - vowels_loss: 0.0715 - consonants_loss: 0.0460 - roots_acc: 0.9665 - vowels_acc: 0.9784 - consonants_acc: 0.9862Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 273us/sample - loss: 0.2628 - roots_loss: 0.1772 - vowels_loss: 0.0912 - consonants_loss: 0.0425 - roots_acc: 0.9534 - vowels_acc: 0.9741 - consonants_acc: 0.9888\n",
            "180/180 [==============================] - 45s 251ms/step - loss: 0.2280 - roots_loss: 0.1104 - vowels_loss: 0.0717 - consonants_loss: 0.0460 - roots_acc: 0.9664 - vowels_acc: 0.9784 - consonants_acc: 0.9862 - val_loss: 0.3109 - val_roots_loss: 0.1772 - val_vowels_loss: 0.0912 - val_consonants_loss: 0.0425 - val_roots_acc: 0.9534 - val_vowels_acc: 0.9741 - val_consonants_acc: 0.9888\n",
            "Epoch 20/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2357 - roots_loss: 0.1115 - vowels_loss: 0.0758 - consonants_loss: 0.0483 - roots_acc: 0.9669 - vowels_acc: 0.9780 - consonants_acc: 0.9855Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 273us/sample - loss: 0.2331 - roots_loss: 0.1754 - vowels_loss: 0.0711 - consonants_loss: 0.0406 - roots_acc: 0.9549 - vowels_acc: 0.9838 - consonants_acc: 0.9900\n",
            "180/180 [==============================] - 45s 250ms/step - loss: 0.2354 - roots_loss: 0.1114 - vowels_loss: 0.0757 - consonants_loss: 0.0483 - roots_acc: 0.9669 - vowels_acc: 0.9780 - consonants_acc: 0.9855 - val_loss: 0.2871 - val_roots_loss: 0.1754 - val_vowels_loss: 0.0711 - val_consonants_loss: 0.0406 - val_roots_acc: 0.9549 - val_vowels_acc: 0.9838 - val_consonants_acc: 0.9900\n",
            "Epoch 21/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2253 - roots_loss: 0.1059 - vowels_loss: 0.0752 - consonants_loss: 0.0442 - roots_acc: 0.9675 - vowels_acc: 0.9779 - consonants_acc: 0.9871Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 272us/sample - loss: 0.2363 - roots_loss: 0.1753 - vowels_loss: 0.0708 - consonants_loss: 0.0403 - roots_acc: 0.9532 - vowels_acc: 0.9838 - consonants_acc: 0.9908\n",
            "\n",
            "Epoch 00021: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
            "180/180 [==============================] - 45s 250ms/step - loss: 0.2249 - roots_loss: 0.1060 - vowels_loss: 0.0749 - consonants_loss: 0.0441 - roots_acc: 0.9675 - vowels_acc: 0.9779 - consonants_acc: 0.9871 - val_loss: 0.2864 - val_roots_loss: 0.1753 - val_vowels_loss: 0.0708 - val_consonants_loss: 0.0403 - val_roots_acc: 0.9532 - val_vowels_acc: 0.9838 - val_consonants_acc: 0.9908\n",
            "Epoch 22/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2222 - roots_loss: 0.1075 - vowels_loss: 0.0691 - consonants_loss: 0.0456 - roots_acc: 0.9672 - vowels_acc: 0.9791 - consonants_acc: 0.9867Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 271us/sample - loss: 0.2386 - roots_loss: 0.1734 - vowels_loss: 0.0698 - consonants_loss: 0.0401 - roots_acc: 0.9532 - vowels_acc: 0.9833 - consonants_acc: 0.9898\n",
            "180/180 [==============================] - 45s 251ms/step - loss: 0.2225 - roots_loss: 0.1076 - vowels_loss: 0.0692 - consonants_loss: 0.0457 - roots_acc: 0.9672 - vowels_acc: 0.9791 - consonants_acc: 0.9867 - val_loss: 0.2834 - val_roots_loss: 0.1734 - val_vowels_loss: 0.0698 - val_consonants_loss: 0.0401 - val_roots_acc: 0.9532 - val_vowels_acc: 0.9833 - val_consonants_acc: 0.9898\n",
            "Epoch 23/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2219 - roots_loss: 0.1051 - vowels_loss: 0.0732 - consonants_loss: 0.0436 - roots_acc: 0.9684 - vowels_acc: 0.9787 - consonants_acc: 0.9874Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 273us/sample - loss: 0.2447 - roots_loss: 0.1752 - vowels_loss: 0.0738 - consonants_loss: 0.0410 - roots_acc: 0.9522 - vowels_acc: 0.9828 - consonants_acc: 0.9898\n",
            "180/180 [==============================] - 45s 250ms/step - loss: 0.2222 - roots_loss: 0.1052 - vowels_loss: 0.0733 - consonants_loss: 0.0437 - roots_acc: 0.9684 - vowels_acc: 0.9788 - consonants_acc: 0.9873 - val_loss: 0.2900 - val_roots_loss: 0.1752 - val_vowels_loss: 0.0738 - val_consonants_loss: 0.0410 - val_roots_acc: 0.9522 - val_vowels_acc: 0.9828 - val_consonants_acc: 0.9898\n",
            "Epoch 24/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2159 - roots_loss: 0.1016 - vowels_loss: 0.0700 - consonants_loss: 0.0442 - roots_acc: 0.9687 - vowels_acc: 0.9793 - consonants_acc: 0.9867Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 273us/sample - loss: 0.2388 - roots_loss: 0.1746 - vowels_loss: 0.0690 - consonants_loss: 0.0392 - roots_acc: 0.9542 - vowels_acc: 0.9836 - consonants_acc: 0.9903\n",
            "180/180 [==============================] - 45s 251ms/step - loss: 0.2157 - roots_loss: 0.1015 - vowels_loss: 0.0701 - consonants_loss: 0.0441 - roots_acc: 0.9687 - vowels_acc: 0.9793 - consonants_acc: 0.9867 - val_loss: 0.2827 - val_roots_loss: 0.1746 - val_vowels_loss: 0.0690 - val_consonants_loss: 0.0392 - val_roots_acc: 0.9542 - val_vowels_acc: 0.9836 - val_consonants_acc: 0.9903\n",
            "Epoch 25/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2098 - roots_loss: 0.0982 - vowels_loss: 0.0669 - consonants_loss: 0.0447 - roots_acc: 0.9706 - vowels_acc: 0.9805 - consonants_acc: 0.9866Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 276us/sample - loss: 0.2379 - roots_loss: 0.1767 - vowels_loss: 0.0688 - consonants_loss: 0.0407 - roots_acc: 0.9539 - vowels_acc: 0.9836 - consonants_acc: 0.9910\n",
            "180/180 [==============================] - 45s 251ms/step - loss: 0.2097 - roots_loss: 0.0983 - vowels_loss: 0.0667 - consonants_loss: 0.0447 - roots_acc: 0.9705 - vowels_acc: 0.9806 - consonants_acc: 0.9866 - val_loss: 0.2863 - val_roots_loss: 0.1767 - val_vowels_loss: 0.0688 - val_consonants_loss: 0.0407 - val_roots_acc: 0.9539 - val_vowels_acc: 0.9836 - val_consonants_acc: 0.9910\n",
            "Epoch 26/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2126 - roots_loss: 0.0998 - vowels_loss: 0.0691 - consonants_loss: 0.0438 - roots_acc: 0.9687 - vowels_acc: 0.9796 - consonants_acc: 0.9873Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 277us/sample - loss: 0.2447 - roots_loss: 0.1776 - vowels_loss: 0.0703 - consonants_loss: 0.0407 - roots_acc: 0.9542 - vowels_acc: 0.9841 - consonants_acc: 0.9908\n",
            "\n",
            "Epoch 00026: ReduceLROnPlateau reducing learning rate to 1e-05.\n",
            "180/180 [==============================] - 45s 250ms/step - loss: 0.2128 - roots_loss: 0.0999 - vowels_loss: 0.0690 - consonants_loss: 0.0438 - roots_acc: 0.9687 - vowels_acc: 0.9796 - consonants_acc: 0.9873 - val_loss: 0.2886 - val_roots_loss: 0.1776 - val_vowels_loss: 0.0703 - val_consonants_loss: 0.0407 - val_roots_acc: 0.9542 - val_vowels_acc: 0.9841 - val_consonants_acc: 0.9908\n",
            "Epoch 27/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2070 - roots_loss: 0.0964 - vowels_loss: 0.0695 - consonants_loss: 0.0410 - roots_acc: 0.9694 - vowels_acc: 0.9796 - consonants_acc: 0.9878Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 273us/sample - loss: 0.2487 - roots_loss: 0.1771 - vowels_loss: 0.0745 - consonants_loss: 0.0418 - roots_acc: 0.9549 - vowels_acc: 0.9831 - consonants_acc: 0.9905\n",
            "180/180 [==============================] - 45s 251ms/step - loss: 0.2065 - roots_loss: 0.0961 - vowels_loss: 0.0694 - consonants_loss: 0.0409 - roots_acc: 0.9695 - vowels_acc: 0.9797 - consonants_acc: 0.9879 - val_loss: 0.2934 - val_roots_loss: 0.1771 - val_vowels_loss: 0.0745 - val_consonants_loss: 0.0418 - val_roots_acc: 0.9549 - val_vowels_acc: 0.9831 - val_consonants_acc: 0.9905\n",
            "Epoch 28/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2047 - roots_loss: 0.0969 - vowels_loss: 0.0665 - consonants_loss: 0.0413 - roots_acc: 0.9711 - vowels_acc: 0.9798 - consonants_acc: 0.9875Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 280us/sample - loss: 0.2409 - roots_loss: 0.1751 - vowels_loss: 0.0698 - consonants_loss: 0.0404 - roots_acc: 0.9549 - vowels_acc: 0.9833 - consonants_acc: 0.9905\n",
            "180/180 [==============================] - 45s 252ms/step - loss: 0.2046 - roots_loss: 0.0968 - vowels_loss: 0.0667 - consonants_loss: 0.0412 - roots_acc: 0.9711 - vowels_acc: 0.9797 - consonants_acc: 0.9875 - val_loss: 0.2853 - val_roots_loss: 0.1751 - val_vowels_loss: 0.0698 - val_consonants_loss: 0.0404 - val_roots_acc: 0.9549 - val_vowels_acc: 0.9833 - val_consonants_acc: 0.9905\n",
            "Epoch 29/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2074 - roots_loss: 0.0951 - vowels_loss: 0.0689 - consonants_loss: 0.0435 - roots_acc: 0.9705 - vowels_acc: 0.9795 - consonants_acc: 0.9873Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 280us/sample - loss: 0.2473 - roots_loss: 0.1769 - vowels_loss: 0.0729 - consonants_loss: 0.0411 - roots_acc: 0.9542 - vowels_acc: 0.9838 - consonants_acc: 0.9905\n",
            "180/180 [==============================] - 45s 252ms/step - loss: 0.2075 - roots_loss: 0.0951 - vowels_loss: 0.0688 - consonants_loss: 0.0436 - roots_acc: 0.9705 - vowels_acc: 0.9795 - consonants_acc: 0.9873 - val_loss: 0.2909 - val_roots_loss: 0.1769 - val_vowels_loss: 0.0729 - val_consonants_loss: 0.0411 - val_roots_acc: 0.9542 - val_vowels_acc: 0.9838 - val_consonants_acc: 0.9905\n",
            "Epoch 30/30\n",
            "179/180 [============================>.] - ETA: 0s - loss: 0.2078 - roots_loss: 0.0968 - vowels_loss: 0.0682 - consonants_loss: 0.0428 - roots_acc: 0.9703 - vowels_acc: 0.9801 - consonants_acc: 0.9878Epoch 1/30\n",
            "4017/180 [=============================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================================] - 1s 277us/sample - loss: 0.2389 - roots_loss: 0.1760 - vowels_loss: 0.0719 - consonants_loss: 0.0406 - roots_acc: 0.9552 - vowels_acc: 0.9841 - consonants_acc: 0.9905\n",
            "180/180 [==============================] - 45s 253ms/step - loss: 0.2077 - roots_loss: 0.0969 - vowels_loss: 0.0681 - consonants_loss: 0.0427 - roots_acc: 0.9702 - vowels_acc: 0.9801 - consonants_acc: 0.9878 - val_loss: 0.2886 - val_roots_loss: 0.1760 - val_vowels_loss: 0.0719 - val_consonants_loss: 0.0406 - val_roots_acc: 0.9552 - val_vowels_acc: 0.9841 - val_consonants_acc: 0.9905\n"
          ]
        }
      ],
      "execution_count": 35,
      "metadata": {
        "id": "70mc-ghmtNa2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "db8f389836774c6a815e1eb8369b5796",
            "9a215952bf6f419492b8d7f77802a28e",
            "7752f406a6e14e619a343b1c0b6a9fac",
            "dfd65153d26941c0a0cd049d145a04bb",
            "39b6924fbeab4213913bdaa10a770f05",
            "e01becd42b1643348cec478be693ff1a",
            "e5f42ba625ef4306b2b1e84a7b6c4128",
            "0c84c97caec84c78abd9a1c6583ab4d5",
            "271da1772d014b7eb0b661e0f554858f",
            "c5b45fab41734acdb79acf956b8d0c82",
            "a87057f7614744aebd86c7f541302c87",
            "743a252125b34e759b62630493afc5cc",
            "87cb78a8e2724781baecd699caf60866",
            "5e0df46176c0498da8adbe19fa55c905",
            "24c01e71757b4cde871af16daaf9ace0",
            "0bf40ea4f07b4570a745b97f708772b2",
            "12f5900e2df345e6bafa3f88136af10b",
            "67336e33df9e442895eba5bed7143043",
            "30a00ca842e54bb0a9ad40e26ff25c97",
            "c10e2d1816324726b118f5ad9963cd08",
            "d98c082597414abf957b69a07aba61d6",
            "d11666ca7d96447d97d77f0ca28eefce",
            "0295a31313a44efd9fb7fe130a4ff482",
            "2f150d021b7c425c8587ea51a47af42f"
          ]
        },
        "outputId": "b02f6e6e-40be-4ffe-e95e-1a9224d7b706"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "model_name = 'ResNet_0303.h5'\n",
        "save_dir = os.path.join(\"drive/My Drive/Colab Notebooks/playground/\", 'saved_models')\n",
        "# Save model and weights\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "model.save(model_path)\n",
        "print('Saved trained model at %s ' % model_path)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved trained model at drive/My Drive/Colab Notebooks/playground/saved_models/ResNet_0303.h5 \n"
          ]
        }
      ],
      "execution_count": 36,
      "metadata": {
        "id": "9mqwrzPptX12",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b09c9af1-f04d-4a98-cacb-0700734d5378"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "id": "hO7Qhm8rExBw",
        "colab_type": "code",
        "colab": {}
      }
    }
  ],
  "metadata": {
    "colab": {
      "name": "ResNet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0295a31313a44efd9fb7fe130a4ff482": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "12f5900e2df345e6bafa3f88136af10b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_model_module": "@jupyter-widgets/controls",
            "_view_name": "HBoxView",
            "_view_module": "@jupyter-widgets/controls",
            "_dom_classes": [],
            "layout": "IPY_MODEL_67336e33df9e442895eba5bed7143043",
            "_view_count": null,
            "_model_module_version": "1.5.0",
            "box_style": "",
            "children": [
              "IPY_MODEL_30a00ca842e54bb0a9ad40e26ff25c97",
              "IPY_MODEL_c10e2d1816324726b118f5ad9963cd08"
            ]
          }
        },
        "5e0df46176c0498da8adbe19fa55c905": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "grid_row": null,
            "_model_module": "@jupyter-widgets/base",
            "overflow": null,
            "max_height": null,
            "display": null,
            "grid_auto_flow": null,
            "grid_template_rows": null,
            "align_self": null,
            "grid_auto_columns": null,
            "width": null,
            "grid_area": null,
            "align_items": null,
            "_view_name": "LayoutView",
            "left": null,
            "height": null,
            "_view_module": "@jupyter-widgets/base",
            "object_position": null,
            "justify_content": null,
            "bottom": null,
            "max_width": null,
            "border": null,
            "margin": null,
            "order": null,
            "grid_column": null,
            "grid_auto_rows": null,
            "padding": null,
            "grid_template_columns": null,
            "justify_items": null,
            "object_fit": null,
            "visibility": null,
            "_view_count": null,
            "flex_flow": null,
            "min_height": null,
            "top": null,
            "min_width": null,
            "flex": null,
            "_model_module_version": "1.2.0",
            "grid_template_areas": null,
            "overflow_x": null,
            "right": null,
            "overflow_y": null,
            "grid_gap": null,
            "align_content": null
          }
        },
        "d98c082597414abf957b69a07aba61d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "ProgressStyleModel",
            "_model_module": "@jupyter-widgets/controls",
            "description_width": "",
            "_view_name": "StyleView",
            "_view_module": "@jupyter-widgets/base",
            "_view_count": null,
            "bar_color": null,
            "_model_module_version": "1.5.0"
          }
        },
        "67336e33df9e442895eba5bed7143043": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "grid_row": null,
            "_model_module": "@jupyter-widgets/base",
            "overflow": null,
            "max_height": null,
            "display": null,
            "grid_auto_flow": null,
            "grid_template_rows": null,
            "align_self": null,
            "grid_auto_columns": null,
            "width": null,
            "grid_area": null,
            "align_items": null,
            "_view_name": "LayoutView",
            "left": null,
            "height": null,
            "_view_module": "@jupyter-widgets/base",
            "object_position": null,
            "justify_content": null,
            "bottom": null,
            "max_width": null,
            "border": null,
            "margin": null,
            "order": null,
            "grid_column": null,
            "grid_auto_rows": null,
            "padding": null,
            "grid_template_columns": null,
            "justify_items": null,
            "object_fit": null,
            "visibility": null,
            "_view_count": null,
            "flex_flow": null,
            "min_height": null,
            "top": null,
            "min_width": null,
            "flex": null,
            "_model_module_version": "1.2.0",
            "grid_template_areas": null,
            "overflow_x": null,
            "right": null,
            "overflow_y": null,
            "grid_gap": null,
            "align_content": null
          }
        },
        "e5f42ba625ef4306b2b1e84a7b6c4128": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "743a252125b34e759b62630493afc5cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_name": "HTMLModel",
            "_model_module": "@jupyter-widgets/controls",
            "_view_name": "HTMLView",
            "_view_module": "@jupyter-widgets/controls",
            "_dom_classes": [],
            "layout": "IPY_MODEL_0bf40ea4f07b4570a745b97f708772b2",
            "value": "100% 50210/50210 [02:04&lt;00:00, 404.62it/s]",
            "style": "IPY_MODEL_24c01e71757b4cde871af16daaf9ace0",
            "placeholder": "",
            "_view_count": null,
            "_model_module_version": "1.5.0",
            "description": ""
          }
        },
        "0c84c97caec84c78abd9a1c6583ab4d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "grid_row": null,
            "_model_module": "@jupyter-widgets/base",
            "overflow": null,
            "max_height": null,
            "display": null,
            "grid_auto_flow": null,
            "grid_template_rows": null,
            "align_self": null,
            "grid_auto_columns": null,
            "width": null,
            "grid_area": null,
            "align_items": null,
            "_view_name": "LayoutView",
            "left": null,
            "height": null,
            "_view_module": "@jupyter-widgets/base",
            "object_position": null,
            "justify_content": null,
            "bottom": null,
            "max_width": null,
            "border": null,
            "margin": null,
            "order": null,
            "grid_column": null,
            "grid_auto_rows": null,
            "padding": null,
            "grid_template_columns": null,
            "justify_items": null,
            "object_fit": null,
            "visibility": null,
            "_view_count": null,
            "flex_flow": null,
            "min_height": null,
            "top": null,
            "min_width": null,
            "flex": null,
            "_model_module_version": "1.2.0",
            "grid_template_areas": null,
            "overflow_x": null,
            "right": null,
            "overflow_y": null,
            "grid_gap": null,
            "align_content": null
          }
        },
        "9a215952bf6f419492b8d7f77802a28e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "grid_row": null,
            "_model_module": "@jupyter-widgets/base",
            "overflow": null,
            "max_height": null,
            "display": null,
            "grid_auto_flow": null,
            "grid_template_rows": null,
            "align_self": null,
            "grid_auto_columns": null,
            "width": null,
            "grid_area": null,
            "align_items": null,
            "_view_name": "LayoutView",
            "left": null,
            "height": null,
            "_view_module": "@jupyter-widgets/base",
            "object_position": null,
            "justify_content": null,
            "bottom": null,
            "max_width": null,
            "border": null,
            "margin": null,
            "order": null,
            "grid_column": null,
            "grid_auto_rows": null,
            "padding": null,
            "grid_template_columns": null,
            "justify_items": null,
            "object_fit": null,
            "visibility": null,
            "_view_count": null,
            "flex_flow": null,
            "min_height": null,
            "top": null,
            "min_width": null,
            "flex": null,
            "_model_module_version": "1.2.0",
            "grid_template_areas": null,
            "overflow_x": null,
            "right": null,
            "overflow_y": null,
            "grid_gap": null,
            "align_content": null
          }
        },
        "39b6924fbeab4213913bdaa10a770f05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "ProgressStyleModel",
            "_model_module": "@jupyter-widgets/controls",
            "description_width": "",
            "_view_name": "StyleView",
            "_view_module": "@jupyter-widgets/base",
            "_view_count": null,
            "bar_color": null,
            "_model_module_version": "1.5.0"
          }
        },
        "2f150d021b7c425c8587ea51a47af42f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "grid_row": null,
            "_model_module": "@jupyter-widgets/base",
            "overflow": null,
            "max_height": null,
            "display": null,
            "grid_auto_flow": null,
            "grid_template_rows": null,
            "align_self": null,
            "grid_auto_columns": null,
            "width": null,
            "grid_area": null,
            "align_items": null,
            "_view_name": "LayoutView",
            "left": null,
            "height": null,
            "_view_module": "@jupyter-widgets/base",
            "object_position": null,
            "justify_content": null,
            "bottom": null,
            "max_width": null,
            "border": null,
            "margin": null,
            "order": null,
            "grid_column": null,
            "grid_auto_rows": null,
            "padding": null,
            "grid_template_columns": null,
            "justify_items": null,
            "object_fit": null,
            "visibility": null,
            "_view_count": null,
            "flex_flow": null,
            "min_height": null,
            "top": null,
            "min_width": null,
            "flex": null,
            "_model_module_version": "1.2.0",
            "grid_template_areas": null,
            "overflow_x": null,
            "right": null,
            "overflow_y": null,
            "grid_gap": null,
            "align_content": null
          }
        },
        "db8f389836774c6a815e1eb8369b5796": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_model_module": "@jupyter-widgets/controls",
            "_view_name": "HBoxView",
            "_view_module": "@jupyter-widgets/controls",
            "_dom_classes": [],
            "layout": "IPY_MODEL_9a215952bf6f419492b8d7f77802a28e",
            "_view_count": null,
            "_model_module_version": "1.5.0",
            "box_style": "",
            "children": [
              "IPY_MODEL_7752f406a6e14e619a343b1c0b6a9fac",
              "IPY_MODEL_dfd65153d26941c0a0cd049d145a04bb"
            ]
          }
        },
        "30a00ca842e54bb0a9ad40e26ff25c97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_name": "IntProgressModel",
            "_model_module": "@jupyter-widgets/controls",
            "max": 50210,
            "bar_style": "success",
            "_view_name": "ProgressView",
            "_view_module": "@jupyter-widgets/controls",
            "_dom_classes": [],
            "layout": "IPY_MODEL_d11666ca7d96447d97d77f0ca28eefce",
            "orientation": "horizontal",
            "value": 50210,
            "style": "IPY_MODEL_d98c082597414abf957b69a07aba61d6",
            "min": 0,
            "_view_count": null,
            "_model_module_version": "1.5.0",
            "description": ""
          }
        },
        "e01becd42b1643348cec478be693ff1a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "grid_row": null,
            "_model_module": "@jupyter-widgets/base",
            "overflow": null,
            "max_height": null,
            "display": null,
            "grid_auto_flow": null,
            "grid_template_rows": null,
            "align_self": null,
            "grid_auto_columns": null,
            "width": null,
            "grid_area": null,
            "align_items": null,
            "_view_name": "LayoutView",
            "left": null,
            "height": null,
            "_view_module": "@jupyter-widgets/base",
            "object_position": null,
            "justify_content": null,
            "bottom": null,
            "max_width": null,
            "border": null,
            "margin": null,
            "order": null,
            "grid_column": null,
            "grid_auto_rows": null,
            "padding": null,
            "grid_template_columns": null,
            "justify_items": null,
            "object_fit": null,
            "visibility": null,
            "_view_count": null,
            "flex_flow": null,
            "min_height": null,
            "top": null,
            "min_width": null,
            "flex": null,
            "_model_module_version": "1.2.0",
            "grid_template_areas": null,
            "overflow_x": null,
            "right": null,
            "overflow_y": null,
            "grid_gap": null,
            "align_content": null
          }
        },
        "a87057f7614744aebd86c7f541302c87": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_name": "IntProgressModel",
            "_model_module": "@jupyter-widgets/controls",
            "max": 50210,
            "bar_style": "success",
            "_view_name": "ProgressView",
            "_view_module": "@jupyter-widgets/controls",
            "_dom_classes": [],
            "layout": "IPY_MODEL_5e0df46176c0498da8adbe19fa55c905",
            "orientation": "horizontal",
            "value": 50210,
            "style": "IPY_MODEL_87cb78a8e2724781baecd699caf60866",
            "min": 0,
            "_view_count": null,
            "_model_module_version": "1.5.0",
            "description": ""
          }
        },
        "7752f406a6e14e619a343b1c0b6a9fac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_name": "IntProgressModel",
            "_model_module": "@jupyter-widgets/controls",
            "max": 50210,
            "bar_style": "success",
            "_view_name": "ProgressView",
            "_view_module": "@jupyter-widgets/controls",
            "_dom_classes": [],
            "layout": "IPY_MODEL_e01becd42b1643348cec478be693ff1a",
            "orientation": "horizontal",
            "value": 50210,
            "style": "IPY_MODEL_39b6924fbeab4213913bdaa10a770f05",
            "min": 0,
            "_view_count": null,
            "_model_module_version": "1.5.0",
            "description": ""
          }
        },
        "c10e2d1816324726b118f5ad9963cd08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_name": "HTMLModel",
            "_model_module": "@jupyter-widgets/controls",
            "_view_name": "HTMLView",
            "_view_module": "@jupyter-widgets/controls",
            "_dom_classes": [],
            "layout": "IPY_MODEL_2f150d021b7c425c8587ea51a47af42f",
            "value": "100% 50210/50210 [02:03&lt;00:00, 406.82it/s]",
            "style": "IPY_MODEL_0295a31313a44efd9fb7fe130a4ff482",
            "placeholder": "",
            "_view_count": null,
            "_model_module_version": "1.5.0",
            "description": ""
          }
        },
        "0bf40ea4f07b4570a745b97f708772b2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "grid_row": null,
            "_model_module": "@jupyter-widgets/base",
            "overflow": null,
            "max_height": null,
            "display": null,
            "grid_auto_flow": null,
            "grid_template_rows": null,
            "align_self": null,
            "grid_auto_columns": null,
            "width": null,
            "grid_area": null,
            "align_items": null,
            "_view_name": "LayoutView",
            "left": null,
            "height": null,
            "_view_module": "@jupyter-widgets/base",
            "object_position": null,
            "justify_content": null,
            "bottom": null,
            "max_width": null,
            "border": null,
            "margin": null,
            "order": null,
            "grid_column": null,
            "grid_auto_rows": null,
            "padding": null,
            "grid_template_columns": null,
            "justify_items": null,
            "object_fit": null,
            "visibility": null,
            "_view_count": null,
            "flex_flow": null,
            "min_height": null,
            "top": null,
            "min_width": null,
            "flex": null,
            "_model_module_version": "1.2.0",
            "grid_template_areas": null,
            "overflow_x": null,
            "right": null,
            "overflow_y": null,
            "grid_gap": null,
            "align_content": null
          }
        },
        "271da1772d014b7eb0b661e0f554858f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_model_module": "@jupyter-widgets/controls",
            "_view_name": "HBoxView",
            "_view_module": "@jupyter-widgets/controls",
            "_dom_classes": [],
            "layout": "IPY_MODEL_c5b45fab41734acdb79acf956b8d0c82",
            "_view_count": null,
            "_model_module_version": "1.5.0",
            "box_style": "",
            "children": [
              "IPY_MODEL_a87057f7614744aebd86c7f541302c87",
              "IPY_MODEL_743a252125b34e759b62630493afc5cc"
            ]
          }
        },
        "d11666ca7d96447d97d77f0ca28eefce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "grid_row": null,
            "_model_module": "@jupyter-widgets/base",
            "overflow": null,
            "max_height": null,
            "display": null,
            "grid_auto_flow": null,
            "grid_template_rows": null,
            "align_self": null,
            "grid_auto_columns": null,
            "width": null,
            "grid_area": null,
            "align_items": null,
            "_view_name": "LayoutView",
            "left": null,
            "height": null,
            "_view_module": "@jupyter-widgets/base",
            "object_position": null,
            "justify_content": null,
            "bottom": null,
            "max_width": null,
            "border": null,
            "margin": null,
            "order": null,
            "grid_column": null,
            "grid_auto_rows": null,
            "padding": null,
            "grid_template_columns": null,
            "justify_items": null,
            "object_fit": null,
            "visibility": null,
            "_view_count": null,
            "flex_flow": null,
            "min_height": null,
            "top": null,
            "min_width": null,
            "flex": null,
            "_model_module_version": "1.2.0",
            "grid_template_areas": null,
            "overflow_x": null,
            "right": null,
            "overflow_y": null,
            "grid_gap": null,
            "align_content": null
          }
        },
        "dfd65153d26941c0a0cd049d145a04bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_name": "HTMLModel",
            "_model_module": "@jupyter-widgets/controls",
            "_view_name": "HTMLView",
            "_view_module": "@jupyter-widgets/controls",
            "_dom_classes": [],
            "layout": "IPY_MODEL_0c84c97caec84c78abd9a1c6583ab4d5",
            "value": "100% 50210/50210 [01:32&lt;00:00, 545.57it/s]",
            "style": "IPY_MODEL_e5f42ba625ef4306b2b1e84a7b6c4128",
            "placeholder": "",
            "_view_count": null,
            "_model_module_version": "1.5.0",
            "description": ""
          }
        },
        "24c01e71757b4cde871af16daaf9ace0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "c5b45fab41734acdb79acf956b8d0c82": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "grid_row": null,
            "_model_module": "@jupyter-widgets/base",
            "overflow": null,
            "max_height": null,
            "display": null,
            "grid_auto_flow": null,
            "grid_template_rows": null,
            "align_self": null,
            "grid_auto_columns": null,
            "width": null,
            "grid_area": null,
            "align_items": null,
            "_view_name": "LayoutView",
            "left": null,
            "height": null,
            "_view_module": "@jupyter-widgets/base",
            "object_position": null,
            "justify_content": null,
            "bottom": null,
            "max_width": null,
            "border": null,
            "margin": null,
            "order": null,
            "grid_column": null,
            "grid_auto_rows": null,
            "padding": null,
            "grid_template_columns": null,
            "justify_items": null,
            "object_fit": null,
            "visibility": null,
            "_view_count": null,
            "flex_flow": null,
            "min_height": null,
            "top": null,
            "min_width": null,
            "flex": null,
            "_model_module_version": "1.2.0",
            "grid_template_areas": null,
            "overflow_x": null,
            "right": null,
            "overflow_y": null,
            "grid_gap": null,
            "align_content": null
          }
        },
        "87cb78a8e2724781baecd699caf60866": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_module_version": "1.2.0",
            "_model_name": "ProgressStyleModel",
            "_model_module": "@jupyter-widgets/controls",
            "description_width": "",
            "_view_name": "StyleView",
            "_view_module": "@jupyter-widgets/base",
            "_view_count": null,
            "bar_color": null,
            "_model_module_version": "1.5.0"
          }
        }
      }
    },
    "nteract": {
      "version": "0.21.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}