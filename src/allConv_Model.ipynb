{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "allConv Model.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4df2971383114f3595635327c4891987": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_dfdacdea5a5243878a63f9088d3d4dbb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_65895e03e15040189e7e6292ae8b80c3",
              "IPY_MODEL_cdaa3b2f16db474ab27b76b3fc116065"
            ]
          }
        },
        "dfdacdea5a5243878a63f9088d3d4dbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "65895e03e15040189e7e6292ae8b80c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a57a7862935c4e9db87f127bcff80a5c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 50210,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 50210,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_7759352872b64df2a4ff9519403492bf"
          }
        },
        "cdaa3b2f16db474ab27b76b3fc116065": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_75ba722634fa4d129c9abc4bd5bb481e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 50210/50210 [01:17&lt;00:00, 646.56it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a92ae16ca1cc43ddbf6b77a09d73f18e"
          }
        },
        "a57a7862935c4e9db87f127bcff80a5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "7759352872b64df2a4ff9519403492bf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "75ba722634fa4d129c9abc4bd5bb481e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a92ae16ca1cc43ddbf6b77a09d73f18e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5LD-03u5eOo",
        "colab_type": "code",
        "outputId": "23f3e27b-9edd-4ff6-e0a8-2261d3cebce3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jm46DGqxEJte",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from tqdm.auto import tqdm\n",
        "from glob import glob\n",
        "import time, gc\n",
        "import cv2\n",
        "from tensorflow import keras\n",
        "import matplotlib.image as mpimg\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Model\n",
        "from keras.models import clone_model\n",
        "from keras.layers import Dense,Conv2D,Flatten,MaxPool2D,Dropout,BatchNormalization, Input\n",
        "from keras.optimizers import Adam\n",
        "from keras.callbacks import ReduceLROnPlateau\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import PIL.Image as Image, PIL.ImageDraw as ImageDraw, PIL.ImageFont as ImageFont\n",
        "from matplotlib import pyplot as plt\n",
        "import seaborn as sns\n",
        "import albumentations as A"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rK0uZpcURKVE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TRAIN = ['drive/My Drive/Neural NotWork/Data/train_image_data_0.parquet',\n",
        "         'drive/My Drive/Neural NotWork/Data/train_image_data_1.parquet',\n",
        "         'drive/My Drive/Neural NotWork/Data/train_image_data_2.parquet',\n",
        "         'drive/My Drive/Neural NotWork/Data/train_image_data_3.parquet']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BOMb5cF_Xjru",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "TEST =  ['drive/My Drive/Neural NotWork/Data/test_image_data_0.parquet',\n",
        "         'drive/My Drive/Neural NotWork/Data/test_image_data_1.parquet',\n",
        "         'drive/My Drive/Neural NotWork/Data/test_image_data_2.parquet',\n",
        "         'drive/My Drive/Neural NotWork/Data/test_image_data_3.parquet']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dTMpqysCRKb9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_df_ = pd.read_csv('drive/My Drive/Neural NotWork/Data/train.csv')\n",
        "sample_sub_df = pd.read_csv('drive/My Drive/Neural NotWork/Data/sample_submission.csv')\n",
        "\n",
        "# not really useful, just for human understanding\n",
        "class_map_df = pd.read_csv('drive/My Drive/Neural NotWork/Data/class_map.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "el2In7KzSvDI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# train_X0 = pd.read_parquet(TRAIN[0],'auto')\n",
        "# train_X1 = pd.read_parquet(TRAIN[1],'auto')\n",
        "# train_X2 = pd.read_parquet(TRAIN[2],'auto')\n",
        "# train_X3 = pd.read_parquet(TRAIN[3],'auto')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "StCSjHNnYKDv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "HEIGHT = 137\n",
        "WIDTH = 236"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ggW8kp4hZZFu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resize(df, size=64, need_progress_bar=True):\n",
        "    resized = {}\n",
        "    resize_size=32\n",
        "    angle=0\n",
        "    if need_progress_bar:\n",
        "        for i in tqdm(range(df.shape[0])):\n",
        "            #image = cv2.resize(df.loc[df.index[i]].values.reshape(137,236),(size,size),None,fx=0.5,fy=0.5,interpolation=cv2.INTER_AREA)\n",
        "            image=df.loc[df.index[i]].values.reshape(137,236)\n",
        "            #Centering\n",
        "            image_center = tuple(np.array(image.shape[1::-1]) / 2)\n",
        "            matrix = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n",
        "            image = cv2.warpAffine(image, matrix, image.shape[1::-1], flags=cv2.INTER_LINEAR,\n",
        "                            borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0))\n",
        "            #Scaling\n",
        "            matrix = cv2.getRotationMatrix2D(image_center, 0, 1.0)\n",
        "            image = cv2.warpAffine(image, matrix, image.shape[1::-1], flags=cv2.INTER_LINEAR,\n",
        "                            borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0))\n",
        "            # #Removing Blur\n",
        "            # aug = A.GaussianBlur(p=1.0)\n",
        "            # image = aug(image=image)['image']\n",
        "            # #Noise Removing\n",
        "            # augNoise=A.MultiplicativeNoise(p=1.0)\n",
        "            # image = augNoise(image=image)['image']\n",
        "            # #Removing Distortion\n",
        "            # augDist=A.ElasticTransform(sigma=50, alpha=1, alpha_affine=10, p=1.0)\n",
        "            # image = augDist(image=image)['image']\n",
        "            #Brightness\n",
        "            augBright=A.RandomBrightnessContrast(p=1.0)\n",
        "            image = augBright(image=image)['image']\n",
        "            _, thresh = cv2.threshold(image, 30, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "            contours, _ = cv2.findContours(thresh,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)[-2:]\n",
        "\n",
        "            idx = 0 \n",
        "            ls_xmin = []\n",
        "            ls_ymin = []\n",
        "            ls_xmax = []\n",
        "            ls_ymax = []\n",
        "            for cnt in contours:\n",
        "                idx += 1\n",
        "                x,y,w,h = cv2.boundingRect(cnt)\n",
        "                ls_xmin.append(x)\n",
        "                ls_ymin.append(y)\n",
        "                ls_xmax.append(x + w)\n",
        "                ls_ymax.append(y + h)\n",
        "            xmin = min(ls_xmin)\n",
        "            ymin = min(ls_ymin)\n",
        "            xmax = max(ls_xmax)\n",
        "            ymax = max(ls_ymax)\n",
        "\n",
        "            roi = image[ymin:ymax,xmin:xmax]\n",
        "            resized_roi = cv2.resize(roi, (resize_size, resize_size),interpolation=cv2.INTER_AREA)\n",
        "            #image=affine_image(image)\n",
        "            #image= crop_resize(image)\n",
        "            #image = cv2.resize(image,(size,size),interpolation=cv2.INTER_AREA)\n",
        "            #image=resize_image(image,(64,64))\n",
        "            #image = cv2.resize(image,(size,size),interpolation=cv2.INTER_AREA)\n",
        "            #gaussian_3 = cv2.GaussianBlur(image, (5,5), cv2.BORDER_DEFAULT) #unblur\n",
        "            #image = cv2.addWeighted(image, 1.5, gaussian_3, -0.5, 0, image)\n",
        "            #kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]]) #filter\n",
        "            #image = cv2.filter2D(image, -1, kernel)\n",
        "            #ret,image = cv2.threshold(image, 128, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
        "            resized[df.index[i]] = resized_roi.reshape(-1)\n",
        "    else:\n",
        "        for i in range(df.shape[0]):\n",
        "            #image = cv2.resize(df.loc[df.index[i]].values.reshape(137,236),(size,size),None,fx=0.5,fy=0.5,interpolation=cv2.INTER_AREA)\n",
        "            image=df.loc[df.index[i]].values.reshape(137,236)\n",
        "            image_center = tuple(np.array(image.shape[1::-1]) / 2)\n",
        "            matrix = cv2.getRotationMatrix2D(image_center, angle, 1.0)\n",
        "            image = cv2.warpAffine(image, matrix, image.shape[1::-1], flags=cv2.INTER_LINEAR,\n",
        "                            borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0))\n",
        "            matrix = cv2.getRotationMatrix2D(image_center, 0, 1.0)\n",
        "            image = cv2.warpAffine(image, matrix, image.shape[1::-1], flags=cv2.INTER_LINEAR,\n",
        "                            borderMode=cv2.BORDER_CONSTANT, borderValue=(0, 0, 0))\n",
        "            #Removing Blur\n",
        "            #aug = A.GaussianBlur(p=1.0)\n",
        "            #image = aug(image=image)['image']\n",
        "            #Noise Removing\n",
        "            #augNoise=A.MultiplicativeNoise(p=1.0)\n",
        "            #image = augNoise(image=image)['image']\n",
        "            #Removing Distortion\n",
        "            #augDist=A.ElasticTransform(sigma=50, alpha=1, alpha_affine=10, p=1.0)\n",
        "            #image = augDist(image=image)['image']\n",
        "            #Brightness\n",
        "            augBright=A.RandomBrightnessContrast(p=1.0)\n",
        "            image = augBright(image=image)['image']\n",
        "            _, thresh = cv2.threshold(image, 30, 255, cv2.THRESH_BINARY_INV + cv2.THRESH_OTSU)\n",
        "            contours, _ = cv2.findContours(thresh,cv2.RETR_LIST,cv2.CHAIN_APPROX_SIMPLE)[-2:]\n",
        "\n",
        "            idx = 0 \n",
        "            ls_xmin = []\n",
        "            ls_ymin = []\n",
        "            ls_xmax = []\n",
        "            ls_ymax = []\n",
        "            for cnt in contours:\n",
        "                idx += 1\n",
        "                x,y,w,h = cv2.boundingRect(cnt)\n",
        "                ls_xmin.append(x)\n",
        "                ls_ymin.append(y)\n",
        "                ls_xmax.append(x + w)\n",
        "                ls_ymax.append(y + h)\n",
        "            xmin = min(ls_xmin)\n",
        "            ymin = min(ls_ymin)\n",
        "            xmax = max(ls_xmax)\n",
        "            ymax = max(ls_ymax)\n",
        "\n",
        "            roi = image[ymin:ymax,xmin:xmax]\n",
        "            resized_roi = cv2.resize(roi, (resize_size, resize_size),interpolation=cv2.INTER_AREA)\n",
        "            #image=affine_image(image)\n",
        "            #image= crop_resize(image)\n",
        "            #image = cv2.resize(image,(size,size),interpolation=cv2.INTER_AREA)\n",
        "            #image=resize_image(image,(64,64))\n",
        "            #image = cv2.resize(image,(size,size),interpolation=cv2.INTER_AREA)\n",
        "            #gaussian_3 = cv2.GaussianBlur(image, (5,5), cv2.BORDER_DEFAULT) #unblur\n",
        "            #image = cv2.addWeighted(image, 1.5, gaussian_3, -0.5, 0, image)\n",
        "            #kernel = np.array([[-1,-1,-1], [-1,9,-1], [-1,-1,-1]]) #filter\n",
        "            #image = cv2.filter2D(image, -1, kernel)\n",
        "            #ret,image = cv2.threshold(image, 128, 255, cv2.THRESH_BINARY_INV+cv2.THRESH_OTSU)\n",
        "            resized[df.index[i]] = resized_roi.reshape(-1)\n",
        "    resized = pd.DataFrame(resized).T\n",
        "    return resized"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GTsIPdK-gFLJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_dummies(df):\n",
        "    cols = []\n",
        "    for col in df:\n",
        "        cols.append(pd.get_dummies(df[col].astype(str)))\n",
        "    return pd.concat(cols, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaaIpckmhGyq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "IMG_SIZE=32\n",
        "N_CHANNELS=1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhuEDaa8gSDh",
        "colab_type": "code",
        "outputId": "0f516926-85a7-4ec8-beeb-0bb8b9f51c3d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        }
      },
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten\n",
        "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, GlobalAveragePooling2D, AveragePooling2D\n",
        "\n",
        "inputs = Input(shape = (IMG_SIZE, IMG_SIZE, 1))\n",
        "\n",
        "\n",
        "model = Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 1))(inputs)\n",
        "model = Conv2D(filters=128, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
        "model = BatchNormalization(momentum=0.15)(model)\n",
        "model = MaxPool2D(pool_size=(3, 3), strides= 2)(model)\n",
        "model = BatchNormalization(momentum=0.15)(model)\n",
        "model = Dropout(rate=0.3)(model)\n",
        "\n",
        "model = Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
        "model = Conv2D(filters=256, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
        "model = BatchNormalization(momentum=0.15)(model)\n",
        "model = MaxPool2D(pool_size=(3, 3), strides= 2)(model)\n",
        "model = BatchNormalization(momentum=0.15)(model)\n",
        "model = Dropout(rate=0.3)(model)\n",
        "\n",
        "\n",
        "model = Conv2D(filters=512, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
        "model = Conv2D(filters=512, kernel_size=(3, 3), padding='SAME', activation='relu')(model)\n",
        "model = BatchNormalization(momentum=0.15)(model)\n",
        "\n",
        "model = GlobalAveragePooling2D()(model)\n",
        "\n",
        "head_root = Dense(168, activation = 'softmax', name = 'roots')(model)\n",
        "head_vowel = Dense(11, activation = 'softmax', name = 'vowels')(model)\n",
        "head_consonant = Dense(7, activation = 'softmax', name = 'consonants')(model)\n",
        "\n",
        "model = Model(inputs=inputs, outputs=[head_root, head_vowel, head_consonant])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:66: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:2041: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:148: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4267: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3733: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mWSbxnzxgiZ3",
        "colab_type": "code",
        "outputId": "e90b6a3b-9d2d-4dca-dc17-527413759760",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 1)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 128)  1280        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 128)  147584      conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 128)  512         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 15, 15, 128)  0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 15, 15, 128)  512         max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 15, 15, 128)  0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 15, 15, 256)  295168      dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 15, 15, 256)  590080      conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 15, 15, 256)  1024        conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 7, 7, 256)    0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 7, 7, 256)    1024        max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dropout_2 (Dropout)             (None, 7, 7, 256)    0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 7, 7, 512)    1180160     dropout_2[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 7, 7, 512)    2359808     conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 7, 7, 512)    2048        conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d_1 (Glo (None, 512)          0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "roots (Dense)                   (None, 168)          86184       global_average_pooling2d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "vowels (Dense)                  (None, 11)           5643        global_average_pooling2d_1[0][0] \n",
            "__________________________________________________________________________________________________\n",
            "consonants (Dense)              (None, 7)            3591        global_average_pooling2d_1[0][0] \n",
            "==================================================================================================\n",
            "Total params: 4,674,618\n",
            "Trainable params: 4,672,058\n",
            "Non-trainable params: 2,560\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkeh1H2AhsGa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='nadam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-0jpwUPrk_55",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "learning_rate_reduction_root = ReduceLROnPlateau(monitor='roots_accuracy', \n",
        "                                            patience=3, \n",
        "                                            verbose=1,\n",
        "                                            factor=0.5, \n",
        "                                            min_lr=0.00001)\n",
        "learning_rate_reduction_vowel = ReduceLROnPlateau(monitor='vowels_accuracy', \n",
        "                                            patience=3, \n",
        "                                            verbose=1,\n",
        "                                            factor=0.5, \n",
        "                                            min_lr=0.00001)\n",
        "learning_rate_reduction_consonant = ReduceLROnPlateau(monitor='consonants_accuracy', \n",
        "                                            patience=3, \n",
        "                                            verbose=1,\n",
        "                                            factor=0.5, \n",
        "                                            min_lr=0.00001)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0n7xbe1leCu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#decreased batch size\n",
        "batch_size = 256\n",
        "epochs = 30"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7z66tIucle2r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MultiOutputDataGenerator(keras.preprocessing.image.ImageDataGenerator):\n",
        "\n",
        "    def flow(self,\n",
        "             x,\n",
        "             y=None,\n",
        "             batch_size=32,\n",
        "             shuffle=True,\n",
        "             sample_weight=None,\n",
        "             seed=None,\n",
        "             save_to_dir=None,\n",
        "             save_prefix='',\n",
        "             save_format='png',\n",
        "             subset=None):\n",
        "\n",
        "        targets = None\n",
        "        target_lengths = {}\n",
        "        ordered_outputs = []\n",
        "        for output, target in y.items():\n",
        "            if targets is None:\n",
        "                targets = target\n",
        "            else:\n",
        "                targets = np.concatenate((targets, target), axis=1)\n",
        "            target_lengths[output] = target.shape[1]\n",
        "            ordered_outputs.append(output)\n",
        "\n",
        "\n",
        "        for flowx, flowy in super().flow(x, targets, batch_size=batch_size,\n",
        "                                         shuffle=shuffle):\n",
        "            target_dict = {}\n",
        "            i = 0\n",
        "            for output in ordered_outputs:\n",
        "                target_length = target_lengths[output]\n",
        "                target_dict[output] = flowy[:, i: i + target_length]\n",
        "                i += target_length\n",
        "\n",
        "            yield flowx, target_dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YOtUH_7yChc1",
        "colab_type": "code",
        "outputId": "095041df-cd0e-4018-ec1b-4c09171b2c68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## run this before retrain modified model \n",
        "keras.backend.backend()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'tensorflow'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LzV5jxwDloyv",
        "colab_type": "code",
        "outputId": "43513f7e-c810-4d18-ff64-42b1f6efaac2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "4df2971383114f3595635327c4891987",
            "dfdacdea5a5243878a63f9088d3d4dbb",
            "65895e03e15040189e7e6292ae8b80c3",
            "cdaa3b2f16db474ab27b76b3fc116065",
            "a57a7862935c4e9db87f127bcff80a5c",
            "7759352872b64df2a4ff9519403492bf",
            "75ba722634fa4d129c9abc4bd5bb481e",
            "a92ae16ca1cc43ddbf6b77a09d73f18e"
          ]
        }
      },
      "source": [
        "histories = []\n",
        "for i in range(1):\n",
        "    train_df = pd.merge(pd.read_parquet(f'drive/My Drive/Neural NotWork/Data/train_image_data_{i}.parquet'), train_df_, on='image_id').drop(['image_id'], axis=1)\n",
        "    # Visualize few samples of current training dataset\n",
        "    # fig, ax = plt.subplots(nrows=3, ncols=4, figsize=(16, 8))\n",
        "    # count=0\n",
        "    # for row in ax:\n",
        "    #     for col in row:\n",
        "    #         col.imshow(resize(train_df.drop(['grapheme_root', 'vowel_diacritic', 'consonant_diacritic'], axis=1).iloc[[count]], need_progress_bar=False).values.reshape(IMG_SIZE, IMG_SIZE))\n",
        "    #         count += 1\n",
        "    # plt.show()\n",
        "    \n",
        "    X_train = train_df.drop(['grapheme_root', 'vowel_diacritic', 'consonant_diacritic', 'grapheme'], axis=1)\n",
        "    X_train = resize(X_train)/255\n",
        "    \n",
        "    # CNN takes images in shape `(batch_size, h, w, channels)`, so reshape the images\n",
        "    X_train = X_train.values.reshape(-1, IMG_SIZE, IMG_SIZE, N_CHANNELS)\n",
        "    \n",
        "    Y_train_root = pd.get_dummies(train_df['grapheme_root']).values\n",
        "    Y_train_vowel = pd.get_dummies(train_df['vowel_diacritic']).values\n",
        "    Y_train_consonant = pd.get_dummies(train_df['consonant_diacritic']).values\n",
        "\n",
        "    print(f'Training images: {X_train.shape}')\n",
        "    print(f'Training labels root: {Y_train_root.shape}')\n",
        "    print(f'Training labels vowel: {Y_train_vowel.shape}')\n",
        "    print(f'Training labels consonants: {Y_train_consonant.shape}')\n",
        "\n",
        "    # Divide the data into training and validation set\n",
        "    x_train, x_test, y_train_root, y_test_root, y_train_vowel, y_test_vowel, y_train_consonant, y_test_consonant = train_test_split(X_train, Y_train_root, Y_train_vowel, Y_train_consonant, test_size=0.08, random_state=124)\n",
        "    del train_df\n",
        "    del X_train\n",
        "    del Y_train_root, Y_train_vowel, Y_train_consonant\n",
        "\n",
        "    # Data augmentation for creating more training data\n",
        "    datagen = MultiOutputDataGenerator(\n",
        "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
        "        samplewise_center=False,  # set each sample mean to 0\n",
        "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
        "        samplewise_std_normalization=False,  # divide each input by its std\n",
        "        zca_whitening=False,  # apply ZCA whitening\n",
        "        rotation_range=8,  # randomly rotate images in the range (degrees, 0 to 180)\n",
        "        zoom_range = 0.15, # Randomly zoom image \n",
        "        width_shift_range=0.15,  # randomly shift images horizontally (fraction of total width)\n",
        "        height_shift_range=0.15,  # randomly shift images vertically (fraction of total height)\n",
        "        horizontal_flip=False,  # randomly flip images\n",
        "        vertical_flip=False)  # randomly flip images\n",
        "\n",
        "\n",
        "    # This will just calculate parameters required to augment the given data. This won't perform any augmentations\n",
        "    datagen.fit(x_train)\n",
        "\n",
        "    # Fit the model\n",
        "    history = model.fit_generator(datagen.flow(x_train, {'roots': y_train_root, 'vowels': y_train_vowel, 'consonants': y_train_consonant}, batch_size=batch_size),\n",
        "                              epochs = epochs, validation_data = (x_test, [y_test_root, y_test_vowel, y_test_consonant]), \n",
        "                              steps_per_epoch=x_train.shape[0] // batch_size, \n",
        "                              callbacks=[learning_rate_reduction_root, learning_rate_reduction_vowel, learning_rate_reduction_consonant])\n",
        "\n",
        "    histories.append(history)\n",
        "    \n",
        "    # Delete to reduce memory usage\n",
        "    del x_train\n",
        "    del x_test\n",
        "    del y_train_root\n",
        "    del y_test_root\n",
        "    del y_train_vowel\n",
        "    del y_test_vowel\n",
        "    del y_train_consonant\n",
        "    del y_test_consonant\n",
        "    gc.collect()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4df2971383114f3595635327c4891987",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=50210), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Training images: (50210, 32, 32, 1)\n",
            "Training labels root: (50210, 168)\n",
            "Training labels vowel: (50210, 11)\n",
            "Training labels consonants: (50210, 7)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "Epoch 1/30\n",
            "180/180 [==============================] - 67s 374ms/step - loss: 6.4896 - roots_loss: 4.1011 - vowels_loss: 1.3862 - consonants_loss: 1.0023 - roots_acc: 0.1185 - vowels_acc: 0.5404 - consonants_acc: 0.6721 - val_loss: 3.7630 - val_roots_loss: 2.5702 - val_vowels_loss: 0.6607 - val_consonants_loss: 0.5320 - val_roots_acc: 0.3408 - val_vowels_acc: 0.7765 - val_consonants_acc: 0.8297\n",
            "Epoch 2/30\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1379: RuntimeWarning: Reduce LR on plateau conditioned on metric `roots_accuracy` which is not available. Available metrics are: val_loss,val_roots_loss,val_vowels_loss,val_consonants_loss,val_roots_acc,val_vowels_acc,val_consonants_acc,loss,roots_loss,vowels_loss,consonants_loss,roots_acc,vowels_acc,consonants_acc,lr\n",
            "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n",
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1379: RuntimeWarning: Reduce LR on plateau conditioned on metric `vowels_accuracy` which is not available. Available metrics are: val_loss,val_roots_loss,val_vowels_loss,val_consonants_loss,val_roots_acc,val_vowels_acc,val_consonants_acc,loss,roots_loss,vowels_loss,consonants_loss,roots_acc,vowels_acc,consonants_acc,lr\n",
            "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n",
            "/usr/local/lib/python3.6/dist-packages/keras/callbacks.py:1379: RuntimeWarning: Reduce LR on plateau conditioned on metric `consonants_accuracy` which is not available. Available metrics are: val_loss,val_roots_loss,val_vowels_loss,val_consonants_loss,val_roots_acc,val_vowels_acc,val_consonants_acc,loss,roots_loss,vowels_loss,consonants_loss,roots_acc,vowels_acc,consonants_acc,lr\n",
            "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "180/180 [==============================] - 56s 313ms/step - loss: 2.7447 - roots_loss: 1.7167 - vowels_loss: 0.5913 - consonants_loss: 0.4368 - roots_acc: 0.5532 - vowels_acc: 0.8077 - consonants_acc: 0.8547 - val_loss: 1.3923 - val_roots_loss: 0.8564 - val_vowels_loss: 0.2828 - val_consonants_loss: 0.2531 - val_roots_acc: 0.7588 - val_vowels_acc: 0.9134 - val_consonants_acc: 0.9178\n",
            "Epoch 3/30\n",
            "180/180 [==============================] - 54s 302ms/step - loss: 1.7310 - roots_loss: 1.0005 - vowels_loss: 0.4201 - consonants_loss: 0.3104 - roots_acc: 0.7272 - vowels_acc: 0.8633 - consonants_acc: 0.8981 - val_loss: 1.0782 - val_roots_loss: 0.6394 - val_vowels_loss: 0.2413 - val_consonants_loss: 0.1976 - val_roots_acc: 0.8158 - val_vowels_acc: 0.9223 - val_consonants_acc: 0.9403\n",
            "Epoch 4/30\n",
            "180/180 [==============================] - 54s 302ms/step - loss: 1.3857 - roots_loss: 0.7753 - vowels_loss: 0.3473 - consonants_loss: 0.2630 - roots_acc: 0.7823 - vowels_acc: 0.8876 - consonants_acc: 0.9128 - val_loss: 0.8640 - val_roots_loss: 0.5004 - val_vowels_loss: 0.2041 - val_consonants_loss: 0.1595 - val_roots_acc: 0.8529 - val_vowels_acc: 0.9345 - val_consonants_acc: 0.9495\n",
            "Epoch 5/30\n",
            "180/180 [==============================] - 54s 301ms/step - loss: 1.1970 - roots_loss: 0.6489 - vowels_loss: 0.3104 - consonants_loss: 0.2378 - roots_acc: 0.8158 - vowels_acc: 0.8998 - consonants_acc: 0.9215 - val_loss: 0.8486 - val_roots_loss: 0.4887 - val_vowels_loss: 0.1927 - val_consonants_loss: 0.1672 - val_roots_acc: 0.8608 - val_vowels_acc: 0.9388 - val_consonants_acc: 0.9502\n",
            "Epoch 6/30\n",
            "180/180 [==============================] - 54s 301ms/step - loss: 1.0568 - roots_loss: 0.5624 - vowels_loss: 0.2819 - consonants_loss: 0.2125 - roots_acc: 0.8377 - vowels_acc: 0.9086 - consonants_acc: 0.9307 - val_loss: 0.7788 - val_roots_loss: 0.4467 - val_vowels_loss: 0.1945 - val_consonants_loss: 0.1376 - val_roots_acc: 0.8718 - val_vowels_acc: 0.9395 - val_consonants_acc: 0.9592\n",
            "Epoch 7/30\n",
            "180/180 [==============================] - 54s 301ms/step - loss: 0.9719 - roots_loss: 0.5070 - vowels_loss: 0.2628 - consonants_loss: 0.2020 - roots_acc: 0.8523 - vowels_acc: 0.9158 - consonants_acc: 0.9335 - val_loss: 0.7010 - val_roots_loss: 0.3974 - val_vowels_loss: 0.1604 - val_consonants_loss: 0.1432 - val_roots_acc: 0.8875 - val_vowels_acc: 0.9492 - val_consonants_acc: 0.9572\n",
            "Epoch 8/30\n",
            "180/180 [==============================] - 54s 301ms/step - loss: 0.8896 - roots_loss: 0.4600 - vowels_loss: 0.2468 - consonants_loss: 0.1828 - roots_acc: 0.8648 - vowels_acc: 0.9212 - consonants_acc: 0.9400 - val_loss: 0.6602 - val_roots_loss: 0.3895 - val_vowels_loss: 0.1416 - val_consonants_loss: 0.1291 - val_roots_acc: 0.8847 - val_vowels_acc: 0.9584 - val_consonants_acc: 0.9602\n",
            "Epoch 9/30\n",
            "180/180 [==============================] - 54s 301ms/step - loss: 0.8190 - roots_loss: 0.4185 - vowels_loss: 0.2315 - consonants_loss: 0.1690 - roots_acc: 0.8757 - vowels_acc: 0.9261 - consonants_acc: 0.9437 - val_loss: 0.6571 - val_roots_loss: 0.3762 - val_vowels_loss: 0.1514 - val_consonants_loss: 0.1294 - val_roots_acc: 0.8930 - val_vowels_acc: 0.9577 - val_consonants_acc: 0.9637\n",
            "Epoch 10/30\n",
            "180/180 [==============================] - 54s 301ms/step - loss: 0.7738 - roots_loss: 0.3856 - vowels_loss: 0.2199 - consonants_loss: 0.1683 - roots_acc: 0.8849 - vowels_acc: 0.9276 - consonants_acc: 0.9445 - val_loss: 0.6548 - val_roots_loss: 0.3769 - val_vowels_loss: 0.1481 - val_consonants_loss: 0.1298 - val_roots_acc: 0.8900 - val_vowels_acc: 0.9564 - val_consonants_acc: 0.9607\n",
            "Epoch 11/30\n",
            "180/180 [==============================] - 54s 301ms/step - loss: 0.7269 - roots_loss: 0.3619 - vowels_loss: 0.2085 - consonants_loss: 0.1565 - roots_acc: 0.8909 - vowels_acc: 0.9331 - consonants_acc: 0.9486 - val_loss: 0.6580 - val_roots_loss: 0.3741 - val_vowels_loss: 0.1439 - val_consonants_loss: 0.1400 - val_roots_acc: 0.8987 - val_vowels_acc: 0.9527 - val_consonants_acc: 0.9579\n",
            "Epoch 12/30\n",
            "180/180 [==============================] - 54s 301ms/step - loss: 0.6725 - roots_loss: 0.3250 - vowels_loss: 0.1966 - consonants_loss: 0.1510 - roots_acc: 0.9020 - vowels_acc: 0.9361 - consonants_acc: 0.9501 - val_loss: 0.6276 - val_roots_loss: 0.3511 - val_vowels_loss: 0.1461 - val_consonants_loss: 0.1304 - val_roots_acc: 0.8999 - val_vowels_acc: 0.9539 - val_consonants_acc: 0.9627\n",
            "Epoch 13/30\n",
            "180/180 [==============================] - 54s 300ms/step - loss: 0.6510 - roots_loss: 0.3116 - vowels_loss: 0.1947 - consonants_loss: 0.1447 - roots_acc: 0.9048 - vowels_acc: 0.9373 - consonants_acc: 0.9511 - val_loss: 0.6465 - val_roots_loss: 0.3616 - val_vowels_loss: 0.1509 - val_consonants_loss: 0.1340 - val_roots_acc: 0.9009 - val_vowels_acc: 0.9525 - val_consonants_acc: 0.9594\n",
            "Epoch 14/30\n",
            "180/180 [==============================] - 54s 300ms/step - loss: 0.6117 - roots_loss: 0.2885 - vowels_loss: 0.1842 - consonants_loss: 0.1390 - roots_acc: 0.9114 - vowels_acc: 0.9403 - consonants_acc: 0.9542 - val_loss: 0.6343 - val_roots_loss: 0.3615 - val_vowels_loss: 0.1438 - val_consonants_loss: 0.1290 - val_roots_acc: 0.8974 - val_vowels_acc: 0.9589 - val_consonants_acc: 0.9619\n",
            "Epoch 15/30\n",
            "180/180 [==============================] - 54s 300ms/step - loss: 0.5736 - roots_loss: 0.2654 - vowels_loss: 0.1789 - consonants_loss: 0.1292 - roots_acc: 0.9196 - vowels_acc: 0.9408 - consonants_acc: 0.9571 - val_loss: 0.6224 - val_roots_loss: 0.3537 - val_vowels_loss: 0.1442 - val_consonants_loss: 0.1246 - val_roots_acc: 0.9024 - val_vowels_acc: 0.9579 - val_consonants_acc: 0.9624\n",
            "Epoch 16/30\n",
            "180/180 [==============================] - 54s 300ms/step - loss: 0.5630 - roots_loss: 0.2635 - vowels_loss: 0.1732 - consonants_loss: 0.1263 - roots_acc: 0.9187 - vowels_acc: 0.9441 - consonants_acc: 0.9582 - val_loss: 0.6224 - val_roots_loss: 0.3532 - val_vowels_loss: 0.1425 - val_consonants_loss: 0.1268 - val_roots_acc: 0.9024 - val_vowels_acc: 0.9559 - val_consonants_acc: 0.9639\n",
            "Epoch 17/30\n",
            "180/180 [==============================] - 54s 300ms/step - loss: 0.5410 - roots_loss: 0.2462 - vowels_loss: 0.1720 - consonants_loss: 0.1229 - roots_acc: 0.9229 - vowels_acc: 0.9440 - consonants_acc: 0.9582 - val_loss: 0.6309 - val_roots_loss: 0.3776 - val_vowels_loss: 0.1221 - val_consonants_loss: 0.1312 - val_roots_acc: 0.8989 - val_vowels_acc: 0.9634 - val_consonants_acc: 0.9619\n",
            "Epoch 18/30\n",
            "180/180 [==============================] - 54s 300ms/step - loss: 0.5137 - roots_loss: 0.2347 - vowels_loss: 0.1611 - consonants_loss: 0.1179 - roots_acc: 0.9273 - vowels_acc: 0.9470 - consonants_acc: 0.9613 - val_loss: 0.6100 - val_roots_loss: 0.3560 - val_vowels_loss: 0.1358 - val_consonants_loss: 0.1182 - val_roots_acc: 0.9066 - val_vowels_acc: 0.9602 - val_consonants_acc: 0.9654\n",
            "Epoch 19/30\n",
            "180/180 [==============================] - 54s 300ms/step - loss: 0.4989 - roots_loss: 0.2247 - vowels_loss: 0.1583 - consonants_loss: 0.1159 - roots_acc: 0.9296 - vowels_acc: 0.9488 - consonants_acc: 0.9612 - val_loss: 0.7100 - val_roots_loss: 0.3694 - val_vowels_loss: 0.2042 - val_consonants_loss: 0.1365 - val_roots_acc: 0.9022 - val_vowels_acc: 0.9305 - val_consonants_acc: 0.9627\n",
            "Epoch 20/30\n",
            " 17/180 [=>............................] - ETA: 47s - loss: 0.5552 - roots_loss: 0.2422 - vowels_loss: 0.1992 - consonants_loss: 0.1139 - roots_acc: 0.9281 - vowels_acc: 0.9347 - consonants_acc: 0.9621"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRg6hzVYlP6J",
        "colab_type": "code",
        "outputId": "98c47702-8fcc-44f7-9df5-20fcc4d9121b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import os\n",
        "model_name = 'allCov_model_0302.h5'\n",
        "save_dir = os.path.join(\"drive/My Drive/Neural NotWork/\", 'saved_models')\n",
        "# Save model and weights\n",
        "if not os.path.isdir(save_dir):\n",
        "    os.makedirs(save_dir)\n",
        "model_path = os.path.join(save_dir, model_name)\n",
        "model.save(model_path)\n",
        "print('Saved trained model at %s ' % model_path)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved trained model at drive/My Drive/Neural NotWork/saved_models/allCov_model_0302.h5 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CrGVFFytmdjh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(figsize=(8,5), dpi=100)\n",
        "plt.plot(history.history['dense_3_accuracy'], label='train_accuracy_root')\n",
        "plt.plot(history.history['dense_4_accuracy'], label='train_accuracy_vowel')\n",
        "plt.plot(history.history['dense_5_accuracy'], label='train_accuracy_consonant')\n",
        "plt.plot(history.history['val_dense_3_accuracy'], label='val_accuracy_root')\n",
        "plt.plot(history.history['val_dense_4_accuracy'], label='val_accuracy_vowel')\n",
        "plt.plot(history.history['val_dense_5_accuracy'], label='val_accuracy_consonant')\n",
        "plt.legend()\n",
        "plt.savefig('2.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPJZvs7zPlwg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}